<!DOCTYPE html><HTML lang="en"><head><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Getting Started · Stheno.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script data-main="../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="../assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="../assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="../assets/themeswap.js"></script><script data-outdated-warner="">function maybeAddWarning () {
    const head = document.getElementsByTagName('head')[0];

    // Add a noindex meta tag (unless one exists) so that search engines don't index this version of the docs.
    if (document.body.querySelector('meta[name="robots"]') === null) {
        const meta = document.createElement('meta');
        meta.name = 'robots';
        meta.content = 'noindex';

        head.appendChild(meta);
    };

    // Add a stylesheet to avoid inline styling
    const style = document.createElement('style');
    style.type = 'text/css';
    style.appendChild(document.createTextNode('.outdated-warning-overlay {  position: fixed;  top: 0;  left: 0;  right: 0;  box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);  z-index: 999;  background-color: #ffaba7;  color: rgba(0, 0, 0, 0.7);  border-bottom: 3px solid #da0b00;  padding: 10px 35px;  text-align: center;  font-size: 15px; }  .outdated-warning-overlay .outdated-warning-closer {    position: absolute;    top: calc(50% - 10px);    right: 18px;    cursor: pointer;    width: 12px; }  .outdated-warning-overlay a {    color: #2e63b8; }    .outdated-warning-overlay a:hover {      color: #363636; }'));
    head.appendChild(style);

    const div = document.createElement('div');
    div.classList.add('outdated-warning-overlay');
    const closer = document.createElement('div');
    closer.classList.add('outdated-warning-closer');

    // Icon by font-awesome (license: https://fontawesome.com/license, link: https://fontawesome.com/icons/times?style=solid)
    closer.innerHTML = '<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times" class="svg-inline--fa fa-times fa-w-11" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 352 512"><path fill="currentColor" d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg>';
    closer.addEventListener('click', function () {
        document.body.removeChild(div);
    });
    let href = '/stable';
    if (window.documenterBaseURL) {
        href = window.documenterBaseURL + '/../stable';
    }
    div.innerHTML = 'This documentation is not for the latest version. <br> <a href="' + href + '">Go to the latest documentation</a>.';
    div.appendChild(closer);
    document.body.appendChild(div);
};

if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', maybeAddWarning);
} else {
    maybeAddWarning();
};
</script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Stheno.jl</span></div><form action="../search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href="">Getting Started</a><ul class="internal"><li><a class="tocitem" href="#Exact-Inference-in-a-GP-in-2-Minutes"><span>Exact Inference in a GP in 2 Minutes</span></a></li><li><a class="tocitem" href="#Fit-a-GP-with-NelderMead-in-2-Minutes"><span>Fit a GP with NelderMead in 2 Minutes</span></a></li><li><a class="tocitem" href="#Fit-a-GP-with-BFGS-in-2-minutes"><span>Fit a GP with BFGS in 2 minutes</span></a></li><li><a class="tocitem" href="#Inference-with-NUTS-in-2-minutes"><span>Inference with NUTS in 2 minutes</span></a></li><li><a class="tocitem" href="#Conclusion"><span>Conclusion</span></a></li></ul></li><li><a class="tocitem" href="../input_types/">Input Types</a></li><li><a class="tocitem" href="../kernel_design/">Kernel Design</a></li><li><a class="tocitem" href="../internals/">Internals</a></li><li><a class="tocitem" href="../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="">Getting Started</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Getting Started</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaGaussianProcesses/Stheno.jl/blob/master/docs/src/getting_started.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="Getting-Started"><a class="docs-heading-anchor" href="#Getting-Started">Getting Started</a><a id="Getting-Started-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-Started" title="Permalink"></a></h1><p>Here we document how to do some basic stuff, including learning and inference in kernel parameters, with Stheno.jl. To do this, we that makes use of a variety of packages from the Julia ecosystem. In particular, we'll make use of</p><ul><li><a href="https://github.com/TuringLang/AdvancedHMC.jl">AdvancedHMC.jl</a> to perform Bayesian inference in our model parameters.</li><li><a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> for point-estimates of our model parameters.</li><li><a href="https://github.com/invenia/ParameterHandling.jl">ParameterHandling.jl</a> to make it easy to work with our model's parameters, and to ensure that it plays nicely with Optim and AdvancedHMC.</li></ul><p>This guide assumes that you know roughly what's going on conceptually with GPs. If you're new to Gaussian processes, I cannot recommend <a href="http://videolectures.net/gpip06_mackay_gpb/">this video lecture</a> highly enough.</p><h2 id="Exact-Inference-in-a-GP-in-2-Minutes"><a class="docs-heading-anchor" href="#Exact-Inference-in-a-GP-in-2-Minutes">Exact Inference in a GP in 2 Minutes</a><a id="Exact-Inference-in-a-GP-in-2-Minutes-1"></a><a class="docs-heading-anchor-permalink" href="#Exact-Inference-in-a-GP-in-2-Minutes" title="Permalink"></a></h2><p>This is only a slightly more interesting version of the first example on the README. It's slightly more interesting in that we give the kernels some learnable parameters.</p><pre><code class="language-julia"># Import the packages we'll need for this bit of the demo.
using AbstractGPs
using Stheno

# Short length-scale and small variance.
l1 = 0.4
s1 = 0.2

# Long length-scale and larger variance.
l2 = 5.0
s2 = 1.0

# Specify a GaussianProcessProbabilisticProgramme object, which is itself a GP
# built from other GPs.
f = @gppp let
    f1 = s1 * stretch(GP(Matern52Kernel()), 1 / l1)
    f2 = s2 * stretch(GP(SEKernel()), 1 / l2)
    f3 = f1 + f2
end;

# Generate a sample from f3, one of the processes in f, at some random input locations.
# Add some iid observation noise, with zero-mean and variance 0.05.
const x = GPPPInput(:f3, collect(range(-5.0, 5.0; length=100)));
σ²_n = 0.05;
fx = f(x, σ²_n);
const y = rand(fx);

# Compute the log marginal likelihood of this observation, just because we can.
logpdf(fx, y)</code></pre><p><code>fx</code> should be thought of as "<code>f</code> at <code>x</code>", and is just as a multivariate Normal distribution, with zero mean and covariance matrix</p><pre><code class="language-julia">cov(f, x) + σ²_n * I</code></pre><p>As such samples can be drawn from it, and the log probability any particular value under it can be computed, in the same way that you would an <code>MvNormal</code> from <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a>.</p><p>We can visualise <code>x</code> and <code>y</code> with <a href="https://github.com/JuliaPlots/Plots.jl">Plots.jl</a></p><pre><code class="language-julia">using Plots
plt = plot();
scatter!(plt, x.x, y; color=:red, label="");
display(plt);</code></pre><p><img alt="img" src="https://willtebbutt.github.io/resources/samples.svg"/></p><p>It's straightforward to compute the posterior over <code>f</code>:</p><pre><code class="language-julia">f_posterior = posterior(fx, y);</code></pre><p><code>f_posterior</code> is another GP, the posterior over <code>f</code> given noisy observations <code>y</code> at inputs <code>x</code>.</p><p>Our plotting recipes aren't quite sophisticated enough at the minute to handle GPPPs properly, but plotting still isn't too much work:</p><pre><code class="language-julia">x_plot = range(-7.0, 7.0; length=1000);
xp = GPPPInput(:f3, x_plot);
ms = marginals(f_posterior(xp));
plot!(
    plt, x_plot, mean.(ms);
    ribbon=3std.(ms), label="", color=:blue, fillalpha=0.2, linewidth=2,
);
plot!(
    plt, x_plot, rand(f_posterior(xp), 10);
    alpha=0.3, label="", color=:blue,
);
display(plt);</code></pre><p><img alt="img" src="https://willtebbutt.github.io/resources/samples_posterior.svg"/></p><p>So you've built a simple GP probabilistic programme, performed inference in it, and looked at the posterior. We've only looked at one component of it though – we could look at others. Consider <code>f2</code>:</p><pre><code class="language-julia">xp2 = GPPPInput(:f2, x_plot);
ms = marginals(f_posterior(xp2));
plot!(
    plt, x_plot, mean.(ms);
    ribbon=3std.(ms), label="", color=:red, fillalpha=0.2, linewidth=2,
);
plot!(
    plt, x_plot, rand(f_posterior(xp2, 1e-9), 10);
    alpha=0.3, label="", color=:red,
);
display(plt);</code></pre><h2 id="Fit-a-GP-with-NelderMead-in-2-Minutes"><a class="docs-heading-anchor" href="#Fit-a-GP-with-NelderMead-in-2-Minutes">Fit a GP with NelderMead in 2 Minutes</a><a id="Fit-a-GP-with-NelderMead-in-2-Minutes-1"></a><a class="docs-heading-anchor-permalink" href="#Fit-a-GP-with-NelderMead-in-2-Minutes" title="Permalink"></a></h2><p>Stheno.jl is slightly unusual in that it declines to provide a <code>fit</code> or <code>train</code> function. Why is this? In short, because there's really no need – the ecosystem now contains everything that is needed to easily do this yourself. By declining to insist on an interface, Stheno.jl is able to interact with a wide array of tools, that you can use in whichever way you please.</p><p>Optim requires that you provide an objective function with a single <code>Vector{&lt;:Real}</code> parameter for most of its optimisers. We'll use ParameterHandling.jl to build one of these in a way that doesn't involve manually writing code to convert between a structured, human-readable, representation of our parameters (in a <code>NamedTuple</code>) and a <code>Vector{Float64}</code>.</p><p>First, we'll put the model from before into a function:</p><pre><code class="language-julia">function build_model(θ::NamedTuple)
    return @gppp let
        f1 = θ.s1 * stretch(GP(SEKernel()), 1 / θ.l1)
        f2 = θ.s2 * stretch(GP(SEKernel()), 1 / θ.l2)
        f3 = f1 + f2
    end
end</code></pre><p>We've assumed that the parameters will be provided as a <code>NamedTuple</code>, so let's build one and check that the model can be constructed:</p><pre><code class="language-julia">using ParameterHandling

θ = (
    # Short length-scale and small variance.
    l1 = positive(0.4),
    s1 = positive(0.2),

    # Long length-scale and larger variance.
    l2 = positive(5.0),
    s2 = positive(1.0),

    # Observation noise variance -- we'll be learning this as well.
    s_noise = positive(0.1),
)</code></pre><p>We've used <code>ParameterHandling.jl</code>s <code>positive</code> constraint to ensure that all of the parameters remain positive during optimisation. Note that there's no magic here, and <code>Optim</code> knows nothing about <code>positive</code>. Rather, <code>ParameterHandling</code> knows how to make sure that <code>Optim</code> will optimise the log of the parameters which we want to be positive.</p><p>We can make this happen with the following:</p><pre><code class="language-julia">using ParameterHandling
using ParameterHandling: value, flatten

θ_flat_init, unflatten = flatten(θ);

# Concrete types used for clarity only.
unpack(θ_flat::Vector{Float64}) = value(unflatten(θ_flat))</code></pre><p>Note that <code>θ_flat_init</code> is a <code>Vector{Float64}</code>, which is usable within <code>Optim</code>. <code>unflatten</code> takes it, and reconstructs <code>θ</code>. Moreover, if we ask for <code>value(θ)</code>, we'll get a version of <code>θ</code> with all of the <code>positive</code> stuff stripped out, and just leave the <code>NamedTuple</code>. So <code>unpack</code> takes the flat form of the parameters, and converts them into the form that <code>build_model</code> is expecting:</p><pre><code class="language-julia">build_model(unpack(θ_flat_init))</code></pre><p>We can now easily define a function which accepts the flat form of the parameters, and return the negative log marginal likelihood of the parameters:</p><pre><code class="language-julia"># nlml = negative log marginal likelihood (of θ)
function nlml(θ_flat)
    θ = unpack(θ_flat)
    f = build_model(θ)
    return -logpdf(f(x, θ.s_noise), y)
end</code></pre><p>We can use any gradient-free optimisation technique from <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> to find the parameters whose negative log marginal likelihood is minimal:</p><pre><code class="language-julia">using Optim
results = Optim.optimize(nlml, θ_flat_init + randn(5), NelderMead())
θ_opt = unpack(results.minimizer);</code></pre><p>Note that we just added some noise to the initial values to make the optimisation more interesting.</p><p>We can now use this to construct the posterior GP and look at the posterior in comparison to the true posterior with the known hyperparameters</p><pre><code class="language-julia">f_opt = build_model(θ_opt);
f_posterior_opt = posterior(f_opt(x, θ_opt.s_noise), y);
ms_opt = marginals(f_posterior_opt(xp));
plot!(
    plt, x_plot, mean.(ms_opt);
    ribbon=3std.(ms_opt), label="", color=:green, fillalpha=0.2, linewidth=2,
);
plot!(
    plt, x_plot, rand(f_posterior_opt(xp, 1e-9), 10);
    alpha=0.3, label="", color=:green,
);
display(plt);</code></pre><p><img alt="img" src="https://willtebbutt.github.io/resources/samples_posterior_both.svg"/></p><p>(Of course the exact posterior has not been recovered because the exact hyperparameters cannot be expected to be recovered given a finite amount of data over a finite width window.)</p><h2 id="Fit-a-GP-with-BFGS-in-2-minutes"><a class="docs-heading-anchor" href="#Fit-a-GP-with-BFGS-in-2-minutes">Fit a GP with BFGS in 2 minutes</a><a id="Fit-a-GP-with-BFGS-in-2-minutes-1"></a><a class="docs-heading-anchor-permalink" href="#Fit-a-GP-with-BFGS-in-2-minutes" title="Permalink"></a></h2><p>The BFGS algorithm is generally the preferred choice when optimising the hyperparameters of fairly simple GPs. It requires access to the gradient of our <code>nlml</code> function, which can be straightforwardly obtained via reverse-mode algorithmic differentiation, which is provided by <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a>:</p><pre><code class="language-julia">using Zygote: gradient

# This will probably take a while to get going as Zygote needs to compile.
results = Optim.optimize(
    nlml,
    θ-&gt;gradient(nlml, θ)[1],
    θ_flat_init + randn(5),
    BFGS(),
    Optim.Options(
        show_trace=true,
    );
    inplace=false,
)
θ_bfgs = unpack(results.minimizer);</code></pre><p>Once more visualising the results:</p><pre><code class="language-julia">f_bfgs = build_model(θ_bfgs);
f_posterior_bfgs = posterior(f_bfgs(x, θ_bfgs.s_noise), y);
ms_bfgs = marginals(f_posterior_bfgs(xp));
plot!(
    plt, x_plot, mean.(ms_bfgs);
    ribbon=3std.(ms_bfgs), label="", color=:orange, fillalpha=0.2, linewidth=2,
);
plot!(
    plt, x_plot, rand(f_posterior_bfgs(xp, 1e-9), 10);
    alpha=0.3, label="", color=:orange,
);
display(plt);</code></pre><p><img alt="img" src="https://willtebbutt.github.io/resources/samples_posterior_bfgs.svg"/></p><p>Notice that the two optimisers produce (almost) indistinguishable results.</p><h2 id="Inference-with-NUTS-in-2-minutes"><a class="docs-heading-anchor" href="#Inference-with-NUTS-in-2-minutes">Inference with NUTS in 2 minutes</a><a id="Inference-with-NUTS-in-2-minutes-1"></a><a class="docs-heading-anchor-permalink" href="#Inference-with-NUTS-in-2-minutes" title="Permalink"></a></h2><p><a href="https://github.com/TuringLang/AdvancedHMC.jl/">AdvancedHMC.jl</a> provides a state-of-the-art implementation of the No-U-Turns sampler, which we can use to perform approximate Bayesian inference in the hyperparameters of the GP. This is slightly longer than the previous examples, but it's all set up associated with AdvancedHMC, which is literally a copy-paste from that package's README:</p><pre><code class="language-julia">using AdvancedHMC, Zygote

# Define the log marginal joint density function and its gradient
ℓπ(θ) = -nlml(θ) - 0.5 * sum(abs2, θ)
function ∂ℓπ∂θ(θ)
    lml, back = Zygote.pullback(ℓπ, θ)
    ∂θ = first(back(1.0))
    return lml, ∂θ
end

# Sampling parameter settings
n_samples, n_adapts = 500, 20

# Draw a random starting points
θ0 = randn(5)

# Define metric space, Hamiltonian, sampling method and adaptor
metric = DiagEuclideanMetric(5)
h = Hamiltonian(metric, ℓπ, ∂ℓπ∂θ)
int = Leapfrog(find_good_eps(h, θ0))
prop = NUTS{MultinomialTS, GeneralisedNoUTurn}(int)
adaptor = StanHMCAdaptor(n_adapts, Preconditioner(metric), NesterovDualAveraging(0.8, int.ϵ))

# Perform inference.
samples, stats = sample(h, prop, θ0, n_samples, adaptor, n_adapts; progress=true)

# Inspect posterior distribution over hyperparameters.
hypers = unpack.(samples);
h_l1 = histogram(getindex.(hypers, :l1); label="l1");
h_l2 = histogram(getindex.(hypers, :l2); label="l2");
h_s1 = histogram(getindex.(hypers, :s1); label="s1");
h_s2 = histogram(getindex.(hypers, :s2); label="s2");
display(plot(h_l1, h_l2, h_s1, h_s2; layout=(2, 2)));</code></pre><p><img alt="img" src="https://willtebbutt.github.io/resources/posterior_hypers.svg"/></p><p>As expected, the sampler converges to the posterior distribution quickly. One could combine this code with that from the previous sections to make predictions under the posterior over the hyperparameters.</p><h2 id="Conclusion"><a class="docs-heading-anchor" href="#Conclusion">Conclusion</a><a id="Conclusion-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusion" title="Permalink"></a></h2><p>So you now know how to fit GPs using Stheno.jl, and to investigate their posterior distributions. It's also straightforward to utilise Stheno.jl inside probabilistic programming frameworks like Soss.jl and Turing.jl (see examples folder).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../input_types/">Input Types »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 19 July 2021 11:42">Monday 19 July 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>