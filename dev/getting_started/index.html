<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Getting Started · Stheno.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Stheno.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li class="current"><a class="toctext" href>Getting Started</a><ul class="internal"><li><a class="toctext" href="#Exact-Inference-in-a-GP-in-2-Minutes-1">Exact Inference in a GP in 2 Minutes</a></li><li><a class="toctext" href="#Fit-a-GP-with-NelderMead-in-2-Minutes-1">Fit a GP with NelderMead in 2 Minutes</a></li><li><a class="toctext" href="#Fit-a-GP-with-BFGS-in-2-minutes-1">Fit a GP with BFGS in 2 minutes</a></li><li><a class="toctext" href="#Inference-with-NUTS-in-2-minutes-1">Inference with NUTS in 2 minutes</a></li><li><a class="toctext" href="#Automation-with-Soss.jl-1">Automation with Soss.jl</a></li><li><a class="toctext" href="#Conclusion-1">Conclusion</a></li></ul></li><li><a class="toctext" href="../kernel_design/">Kernel Design</a></li><li><a class="toctext" href="../internals/">Internals</a></li><li><a class="toctext" href="../input_types/">Input Types</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Getting Started</a></li></ul><a class="edit-page" href="https://github.com/willtebbutt/Stheno.jl/blob/master/docs/src/getting_started.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Getting Started</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Getting-Started-1" href="#Getting-Started-1">Getting Started</a></h1><p>Here we document how to achieve the basic things that any GP package aught to be able to do. We lean heavily on the rest of the Julia ecosystem for each of these examples – this page really exemplifies the way in which different packages play together nicely in the Julia!</p><p>This guide assumes that you know roughly what&#39;s going on conceptually with GPs. If you&#39;re new to Gaussian processes, I cannot recommend <a href="http://videolectures.net/gpip06_mackay_gpb/">this video lecture</a> highly enough.</p><p>We shall first cover the most low-level ways to perform inference in both the process and hyperparameters, and then discuss how to integrate Stheno models with Soss.jl to remove most of the annoying boilerplate code.</p><h2><a class="nav-anchor" id="Exact-Inference-in-a-GP-in-2-Minutes-1" href="#Exact-Inference-in-a-GP-in-2-Minutes-1">Exact Inference in a GP in 2 Minutes</a></h2><p>While Stheno offers some bells and whistles that other GP frameworks do not, it also offers the same functionality as a usual GP framework.</p><pre><code class="language-julia">using Stheno

# Choose the length-scale and variance of the process.
l = 0.4
σ² = 1.3

# Construct a kernel with this variance and length scale.
k = σ² * stretch(matern52(), 1 / l)

# Specify a zero-mean GP with this kernel. Don&#39;t worry about the GPC object.
f = GP(k, GPC())

# Generate a sample from this GP at some random input locations.
# Add some iid observation noise, with zero-mean and variance 0.05.
const x = randn(100)
σ²_n = 0.05
fx = f(x, σ²_n)
const y = rand(fx)

# Compute the log marginal likelihood of this observation, just because we can.
logpdf(fx, y)</code></pre><p><code>fx</code> should be thought of as &quot;<code>f</code> at <code>x</code>&quot;, and is just as a multivariate Normal distribution, with zero mean and covariance matrix</p><pre><code class="language-julia">Stheno.pairwise(k, x) + σ²_n * I</code></pre><p>As such samples can be drawn from it, and the log probability any particular value under it can be computed, in the same way that you would an <code>MvNormal</code> from <a href="https://github.com/JuliaStats/Distributions.jl">Distributions.jl</a>.</p><p>We can visualise <code>x</code> and <code>y</code> with <a href="https://github.com/JuliaPlots/Plots.jl">Plots.jl</a></p><pre><code class="language-julia">using Plots
plt = plot();
scatter!(plt, x, y; color=:red, label=&quot;&quot;);
display(plt);</code></pre><p><img src="https://willtebbutt.github.io/resources/samples.svg" alt="img"/></p><p>It&#39;s straightforward to compute the posterior over <code>f</code>:</p><pre><code class="language-julia">f_posterior = f | Obs(fx, y)</code></pre><p><code>f_posterior</code> is another GP, the posterior over <code>f</code> given noisy observations <code>y</code> at inputs <code>x</code>. Equivalently:</p><pre><code class="language-julia">f_posterior = f | (fx ← y) # ← is \leftarrow[TAB]</code></pre><p>This is just syntactic sugar for the above. You can use it, or not, the choice is entirely your own.</p><p>Stheno.jl knows how to use <a href="https://github.com/JuliaPlots/Plots.jl">Plots.jl</a> to plot GPs, so it&#39;s straightforward to look at the posterior:</p><pre><code class="language-julia">x_plot = range(-4.0, 4.0; length=1000);
plot!(plt, f_posterior(x_plot); samples=10, label=&quot;&quot;, color=:blue);
display(plt);</code></pre><p><img src="https://willtebbutt.github.io/resources/samples_posterior.svg" alt="img"/></p><h2><a class="nav-anchor" id="Fit-a-GP-with-NelderMead-in-2-Minutes-1" href="#Fit-a-GP-with-NelderMead-in-2-Minutes-1">Fit a GP with NelderMead in 2 Minutes</a></h2><p>Stheno.jl is slightly unusual in that it declines to provide a <code>fit</code> or <code>train</code> function. Why is this? In short, because it&#39;s hard to design a one-size-fits-all interface for training a GP that composes well with the rest of the tools in the Julia ecosystem, and you <em>really</em> want to avoid creating any impediments to interacting with other tools in the ecosystem.</p><p>Here we demonstrate the simplest most low-level way to work with Stheno, in which everything is done manually. This example is to demonstrate that the previous section provides all of the basic building blocks that you <em>need</em> to solve regression problems with GPs.</p><pre><code class="language-julia">function unpack(θ)
    σ² = exp(θ[1]) + 1e-6
    l = exp(θ[2]) + 1e-6
    σ²_n = exp(θ[3]) + 1e-6
    return σ², l, σ²_n
end

# nlml = negative log marginal likelihood (of θ)
function nlml(θ)
    σ², l, σ²_n = unpack(θ)
    k = σ² * stretch(matern52(), 1 / l)
    f = GP(k, GPC())
    return -logpdf(f(x, σ²_n), y)
end</code></pre><p>Hopefully it&#39;s clear what we mean by low-level here. We&#39;ve manually defined a function to unpack a parameter vector <code>θ</code> and use this to construct a function that computes the negative log marginal likelihood of <code>θ</code>. We can use any gradient-free optimisation technique from <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> to find the parameters whose negative log marginal likelihood is minimal:</p><pre><code class="language-julia">using Optim
θ0 = randn(3);
results = Optim.optimize(nlml, θ0, NelderMead())
σ²_ml, l_ml, σ²_n_ml = unpack(results.minimizer);</code></pre><p>We can now use this to construct the posterior GP and look at the posterior in comparison to the true posterior with the known hyperparameters</p><pre><code class="language-julia">k = σ²_ml * stretch(matern52(), 1 / l_ml);
f = GP(k, GPC());
f_posterior_ml = f | Obs(f(x, σ²_n_ml), y);
plot!(plt, f_posterior_ml(x_plot); samples=10, color=:green, label=&quot;&quot;);
display(plt);</code></pre><p><img src="https://willtebbutt.github.io/resources/samples_posterior_both.svg" alt="img"/></p><p>(Of course the exact posterior has not been recovered because the exact hyperparameters cannot be expected to be recovered.)</p><h2><a class="nav-anchor" id="Fit-a-GP-with-BFGS-in-2-minutes-1" href="#Fit-a-GP-with-BFGS-in-2-minutes-1">Fit a GP with BFGS in 2 minutes</a></h2><p>The BFGS algorithm is generally the preferred choice when optimising the hyperparameters of fairly simple GPs. It requires access to the gradient of our <code>nlml</code> function, which can be straightforwardly obtained via reverse-mode algorithmic differentiation, which is provided by <a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a>:</p><pre><code class="language-julia">using Zygote: gradient
θ0 = randn(3);
results = Optim.optimize(nlml, θ-&gt;gradient(nlml, θ)[1], θ0, BFGS(); inplace=false)
σ²_bfgs, l_bfgs, σ²_n_bfgs = unpack(results.minimizer);</code></pre><p>Once more visualising the results:</p><pre><code class="language-julia">k = σ²_bfgs * stretch(matern52(), 1 / l_bfgs);
f = GP(k, GPC());
f_posterior_bfgs = f | Obs(f(x, σ²_n_bfgs), y);
plot!(plt, f_posterior_bfgs(x_plot); samples=10, color=:purple, label=&quot;&quot;);
display(plt);</code></pre><p><img src="https://willtebbutt.github.io/resources/samples_posterior_bfgs.svg" alt="img"/></p><p>Notice that the two optimisers produce (almost) indistinguishable results.</p><h2><a class="nav-anchor" id="Inference-with-NUTS-in-2-minutes-1" href="#Inference-with-NUTS-in-2-minutes-1">Inference with NUTS in 2 minutes</a></h2><p><a href="https://github.com/TuringLang/AdvancedHMC.jl/">AdvancedHMC.jl</a> provides a state-of-the-art implementation of the No-U-Turns sampler, which we can use to perform approximate Bayesian inference in the hyperparameters of the GP. This is slightly longer than the previous examples, but it&#39;s all set up associated with AdvancedHMC, which is literally a copy-paste from that package&#39;s README:</p><pre><code class="language-julia">using AdvancedHMC, Zygote

# Define the log marginal likelihood function and its gradient
ℓπ(θ) = -nlml(θ)
function ∂ℓπ∂θ(θ)
    lml, back = Zygote.pullback(ℓπ, θ)
    ∂θ = first(back(1.0))
    return lml, ∂θ
end

# Sampling parameter settings
n_samples, n_adapts = 100, 20

# Draw a random starting points
θ0 = randn(3)

# Define metric space, Hamiltonian, sampling method and adaptor
metric = DiagEuclideanMetric(3)
h = Hamiltonian(metric, ℓπ, ∂ℓπ∂θ)
int = Leapfrog(find_good_eps(h, θ0))
prop = NUTS{MultinomialTS, GeneralisedNoUTurn}(int)
adaptor = StanHMCAdaptor(n_adapts, Preconditioner(metric), NesterovDualAveraging(0.8, int.ϵ))

# Perform inference
samples, stats = sample(h, prop, θ0, n_samples, adaptor, n_adapts; progress=true)

# Inspect posterior distribution over hyperparameters.
hypers = unpack.(samples);
plt_hypers = plot();
plot!(plt_hypers, getindex.(hypers, 1); label=&quot;variance&quot;);
plot!(plt_hypers, getindex.(hypers, 2); label=&quot;length scale&quot;);
plot!(plt_hypers, getindex.(hypers, 3); label=&quot;obs noise variance&quot;);
display(plt_hypers);</code></pre><p><img src="https://willtebbutt.github.io/resources/posterior_hypers.svg" alt="img"/></p><p>As expected, the sampler converges to the posterior distribution quickly. One could combine this code with that from the previous sections to make predictions under the posterior over the hyperparameters.</p><p>Also note that we didn&#39;t specify a prior over the kernel parameters in this example, so essentially used an improper prior. We could have used a proper prior by appropriately modifying <code>ℓπ</code>.</p><h2><a class="nav-anchor" id="Automation-with-Soss.jl-1" href="#Automation-with-Soss.jl-1">Automation with Soss.jl</a></h2><p>In all of the examples above it has been necessary to define the function <code>unpack</code> to convert between a vector representation of our hyperparameters and one that is useful for computing the nlml. Moreover, it has been necessary to manually define conversions between unconstrained and constrained parametrisations of our hyperparameters. While this is fairly straightforward for a small number of parameters, it&#39;s clearly not a scalable solution as models grow in size and complexity.</p><p>Instead we can make use of functionality defined in <a href="https://github.com/cscherrer/Soss.jl">Soss.jl</a> to</p><ul><li>specify priors for the model parameters</li><li>automatically derive transformations between a vector of unconstrained real numbers and a form that Soss models know how to handle</li></ul><h3><a class="nav-anchor" id="Specifying-a-Soss-model-1" href="#Specifying-a-Soss-model-1">Specifying a Soss model</a></h3><pre><code class="language-none">using Soss

M = @model x begin
    σ² ~ LogNormal(0, 1)
    l ~ LogNormal(0, 1)
    σ²_n ~ LogNormal(0, 1)
    f = Stheno.GP(σ² * stretch(matern52(), 1 / l), Stheno.GPC())
    y ~ f(x, σ²_n + 1e-3)
end</code></pre><p>The above defines the model <code>M</code>, with <code>LogNormal(0, 1)</code> priors over each of our hyperparameters.</p><pre><code class="language-none">const t = xform(M(x=x), (y=y,))</code></pre><p><code>t</code> is a function that transforms between a vector of model parameters and a <code>NamedTuple</code> that <code>Soss</code> understands.</p><pre><code class="language-none">function ℓπ(θ)
    (θ_, logjac) = Soss.transform_and_logjac(t, θ)
    return logpdf(M(x=x), merge(θ_, (y=y,))) + logjac
end
nlj(θ) = -ℓπ(θ)</code></pre><p><code>ℓπ</code> computes the log joint of the hyperparameters and data. Note that we have replaced the unpacking and transformation functionality from the previous code with the automatically generated function <code>t</code>.</p><p>The examples that follow simply repeat those found above, but utilise <code>t</code>.</p><h3><a class="nav-anchor" id="Fit-with-Nelder-Mead:-Stheno-Soss-Optim-1" href="#Fit-with-Nelder-Mead:-Stheno-Soss-Optim-1">Fit with Nelder Mead: Stheno + Soss + Optim</a></h3><pre><code class="language-none">θ0 = randn(3);
results = Optim.optimize(nlj, θ0, NelderMead());
θ, _ = Soss.transform_and_logjac(t, results.minimizer);</code></pre><h3><a class="nav-anchor" id="Fit-with-BFGS:-Stheno-Soss-Zygote-Optim-1" href="#Fit-with-BFGS:-Stheno-Soss-Zygote-Optim-1">Fit with BFGS: Stheno + Soss + Zygote + Optim</a></h3><pre><code class="language-none"># Hack to make Zygote play nicely with a particular thing in Distributions.
Zygote.@nograd Distributions.insupport

# Point estimate of parameters via BFGS.
θ0 = randn(3);
results = Optim.optimize(nlj, θ-&gt;first(gradient(nlj, θ)), θ0, BFGS(); inplace=false)
θ, _ = Soss.transform_and_logjac(t, results.minimizer);</code></pre><h3><a class="nav-anchor" id="Inference-with-NUTS:-Stheno-Soss-Zygote-AdvancedHMC-1" href="#Inference-with-NUTS:-Stheno-Soss-Zygote-AdvancedHMC-1">Inference with NUTS: Stheno + Soss + Zygote + AdvancedHMC</a></h3><pre><code class="language-none"># Inference with AdvancedHMC.
function ∂ℓπ∂θ(θ)
    lml, back = Zygote.pullback(ℓπ, θ)
    ∂θ = first(back(1.0))
    return lml, ∂θ
end

# Sampling parameter settings
n_samples, n_adapts = 100, 20

# Draw a random starting points
θ0 = randn(3)

# Define metric space, Hamiltonian, sampling method and adaptor
metric = DiagEuclideanMetric(3)
h = Hamiltonian(metric, ℓπ, ∂ℓπ∂θ)
int = Leapfrog(find_good_eps(h, θ0))
prop = NUTS{MultinomialTS, GeneralisedNoUTurn}(int)
adaptor = StanHMCAdaptor(n_adapts, Preconditioner(metric), NesterovDualAveraging(0.8, int.ϵ))

# Perform inference using NUTS.
samples, stats = sample(h, prop, θ0, n_samples, adaptor, n_adapts; progress=true)

hypers = first.(Soss.transform_and_logjac.(Ref(t), samples));</code></pre><p><code>hypers</code> could be plotted in exactly the same manner as before.</p><h2><a class="nav-anchor" id="Conclusion-1" href="#Conclusion-1">Conclusion</a></h2><p>That&#39;s it! You now know how to do typical GP stuff in Stheno. In particular how to:</p><ul><li>specify a kernel with a particular length-scale and variance</li><li>construct a GP</li><li>sample from a GP, and specify an observation noise</li><li>compute the log marginal likelihood of some observations</li><li>visualise a simple 1D example</li><li>infer kernel parameters in a variety of ways</li><li>utilise Soss.jl to automatically handle parameters and their transformations</li></ul><p>This are just the basic features of Stheno, that you could expect to find in any other GP package. We <em>haven&#39;t</em> covered any of the fancy features of Stheno yet though.</p><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Home</span></a><a class="next" href="../kernel_design/"><span class="direction">Next</span><span class="title">Kernel Design</span></a></footer></article></body></html>
