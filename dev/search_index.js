var documenterSearchIndex = {"docs":
[{"location":"api/#Public-API","page":"API","title":"Public API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CurrentModule = Stheno","category":"page"},{"location":"api/#GPPP","page":"API","title":"GPPP","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Core functionality for working with GPPPs.","category":"page"},{"location":"api/","page":"API","title":"API","text":"@gppp\nGPPPInput\nBlockData\nsplit","category":"page"},{"location":"api/#Stheno.@gppp","page":"API","title":"Stheno.@gppp","text":"@gppp(model_expression)\n\nConstruct a GaussianProcessProbabilisticProgramme (GPPP) from a code snippet.\n\nf = @gppp let\n    f1 = GP(SEKernel())\n    f2 = GP(Matern52Kernel())\n    f3 = f1 + f2\nend\n\nx = GPPPInput(:f3, randn(5))\n\ny = rand(f(x, 0.1))\n\nlogpdf(f(x, 0.1), y) ≈ elbo(f(x, 0.1), y, f(x, 1e-9))\n\n# output\n\ntrue\n\n\n\n\n\n","category":"macro"},{"location":"api/#Stheno.GPPPInput","page":"API","title":"Stheno.GPPPInput","text":"GPPPInput(p, x::AbstractVector)\n\nAn collection of inputs for a GPPP. p indicates which process the vector x should be extracted from. The required type of p is determined by the type of the keys in the GPPP indexed.\n\njulia> x = [1.0, 1.5, 0.3];\n\njulia> v = GPPPInput(:a, x)\n3-element GPPPInput{Symbol, Float64, Vector{Float64}}:\n (:a, 1.0)\n (:a, 1.5)\n (:a, 0.3)\n\njulia> v isa AbstractVector{Tuple{Symbol, Float64}}\ntrue\n\njulia> v == map(x_ -> (:a, x_), x)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/#Stheno.BlockData","page":"API","title":"Stheno.BlockData","text":"BlockData{T, TV<:AbstractVector{T}, TX<:AbstractVector{TV}} <: AbstractVector{T}\n\nA strictly ordered collection of AbstractVectors, representing a ragged array of data.\n\nVery useful when working with GPPPs. For example\n\nf = @gppp let\n    f1 = GP(SEKernel())\n    f2 = GP(Matern52Kernel())\n    f3 = f1 + f2\nend\n\n# Specify a `BlockData` set that can be used to index into\n# the `f2` and `f3` processes in `f`.\nx = BlockData(\n    GPPPInput(:f2, randn(4)),\n    GPPPINput(:f3, randn(3)),\n)\n\n# Index into `f` at the input.\nf(x)\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.split","page":"API","title":"Base.split","text":"Base.split(x::BlockData, Y::AbstractVecOrMat)\n\nConvenience functionality to make working with the output of operations on GPPPs easier. Splits up the rows of Y according to the sizes of the data in x.\n\njulia> Y = vcat(randn(5, 3), randn(4, 3));\n\njulia> x = BlockData(randn(5), randn(4));\n\njulia> Y1, Y2 = split(x, Y);\n\njulia> Y1 == Y[1:5, :]\ntrue\n\njulia> Y2 == Y[6:end, :]\ntrue\n\nWorks with any BlockData, so blocks can e.g. be GPPPInputs. This is particularly helpful for working with the output from rand and marginals from a GPPP indexed at BlockData. For example\n\nf = @gppp let\n    f1 = GP(SEKernel())\n    f2 = GP(Matern52Kernel())\n    f3 = f1 + f2\nend\n\nx = BlockData(GPPPInput(:f2, randn(3)), GPPPInput(:f3, randn(4)))\ny = rand(f(x))\ny2, y3 = split(x, y)\n\nFunctionality also works with any AbstractVector.\n\n\n\n\n\n","category":"function"},{"location":"api/#Building-GPPPs","page":"API","title":"Building GPPPs","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"If in doubt about what any of these transformations can do, play around with them!","category":"page"},{"location":"api/","page":"API","title":"API","text":"+\n*\nstretch\nperiodic\nshift\nselect","category":"page"},{"location":"api/#Base.:+","page":"API","title":"Base.:+","text":"+(fa::AbstractGP, fb::AbstractGP)\n\nProduces an AbstractGP f satisfying f(x) = fa(x) + fb(x).\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.:*","page":"API","title":"Base.:*","text":"*(f, g::AbstractGP)\n\nProduce an AbstractGP h satisfying to h(x) = f(x) * g(x), for some deterministic function f.\n\nIf f isa Real, then h(x) = f * g(x).\n\n\n\n\n\n","category":"function"},{"location":"api/#Stheno.stretch","page":"API","title":"Stheno.stretch","text":"stretch(f::AbstractGP, l::Union{AbstractVecOrMat{<:Real}, Real})\n\nThis is the primary mechanism by which to introduce length scales to your programme.\n\nIf l isa Real or l isa AbstractMatrix{<:Real}, stretch(f, l)(x) == f(l * x) for any input x. In the l isa Real case, this is equivalent to scaling the length scale by 1 / l.\n\nl isa AbstractVector{<:Real} is equivalent to stretch(f, Diagonal(l)).\n\nEquivalent to f ∘ Stretch(l).\n\n\n\n\n\n","category":"function"},{"location":"api/#Stheno.periodic","page":"API","title":"Stheno.periodic","text":"periodic(g::AbstractGP, f::Real)\n\nProduce an AbstractGP with period f.\n\n\n\n\n\n","category":"function"},{"location":"api/#Stheno.shift","page":"API","title":"Stheno.shift","text":"shift(f::AbstractGP, a::Real)\nshift(f::AbstractGP, a::AbstractVector{<:Real})\n\nReturns the GP g given by g(x) = f(x - a)\n\n\n\n\n\n","category":"function"},{"location":"api/#Stheno.select","page":"API","title":"Stheno.select","text":"select(f::AbstractGP, idx)\n\nSelect the dimensions of the input to f given by idx.\n\n\n\n\n\n","category":"function"},{"location":"input_types/#Input-Types","page":"Input Types","title":"Input Types","text":"","category":"section"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"Stheno enables the user to handle any type of input domain they wish and provides a common API that users must implement when considering a new way of representing input data to ensure that the package knows how to handle them appropriately.","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"This interface has now been adopted throughout the JuliaGaussianProcesses ecosystem.","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"Discussed below is this interface's core assumptions, a detailed account of a couple of important concrete input types.","category":"page"},{"location":"input_types/#The-Central-Assumption","page":"Input Types","title":"The Central Assumption","text":"","category":"section"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"The central assumption made in all user-facing and internal functions is this: when a collection of inputs are required, they subtype AbstractVector. For example, x isa AbstractVector when indexing into a GP:","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"f(x, σ²)","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"or computing the covariance matrix associated with a kernel:","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"kernelmatrix(SqExponentialKernel(), x)","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"When computing the cross-covariance matrix between two GPs","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"cov(f, g, x_f, x_g)","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"x_f and x_g must be AbstractVectors. All other operations involving collections of inputs have the same restrictions. Always AbstractVectors.","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"For example, this means that when handling multi-dimensional inputs stored in a Matrix they must be wrapped so that the package knows to treat them as a vector — more on this in below in D-dimensional Euclidean Spaces.","category":"page"},{"location":"input_types/#Dimensional-Euclidean-Space","page":"Input Types","title":"1-Dimensional Euclidean Space","text":"","category":"section"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"When constructing a GP whose domain is the real-line, for example when using a GP to solve some kind of time-series problem, it is sufficient to work with Vector{<:Real}s of inputs. As such, the following is completely valid:","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"using Stheno: GPC\nf = wrap(GP(SqExponentialKernel()), GPC())\nx = randn(10)\nf(x)","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"It is also possible to work with other AbstractArrays that represent a vector of 1-dimensional points, e.g.","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"x = range(-5.0, 5.0; length=100)\nf(x)","category":"page"},{"location":"input_types/#D-Dimensional-Euclidean-Space","page":"Input Types","title":"D-Dimensional Euclidean Space","text":"","category":"section"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"Many applications of interest involve more than a single input-dimension, such as spatio-temporal modeling or Machine Learning tasks. For these cases, we provide ColVecs <: AbstractVector.","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"X_data = randn(5, 100)\nX = ColVecs(X_data)","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"tells Stheno that it should treat each column of X_data as a vector-valued input. Phrased differently, X is an AbstractVector{T} where T <: Vector{<:Real}, which stores its elements in memory as a dense matrix. This approach has the advantage of making it completely explicit how Stheno treats a matrix of data, and also simplifies quite a bit of the internal machinery, as all vectors of inputs can be assumed to be a subtype of AbstractVector.","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"If, on the other hand, each row of X_data corresponds to a vector-valued input, use RowVecs(X_data).","category":"page"},{"location":"input_types/#Structure-in-D-Dimensional-Euclidean-Space","page":"Input Types","title":"Structure in D-Dimensional Euclidean Space","text":"","category":"section"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"Consider a rectilinear grid of points in D-dimensional Euclidean space. Such grids of points can be represented in a more memory-efficient manner than can arbitrarily locate sets of points. Moreover, this structure can be exploited to accelerate inference for certain types of problems dramatically. Other such examples exist e.g., uniform grids in N-dimensions, and can be exploited to more efficiently represent input data and to accelerate inference.","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"Work to exploit these kinds of structures is on-going at the time of writing and will be documented before merging.","category":"page"},{"location":"input_types/#GPPPInput-and-BlockData","page":"Input Types","title":"GPPPInput and BlockData","text":"","category":"section"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"A GPPPInput is a special kind of AbstractVector, specifically designed for GPPPs. Given a GPPP:","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"f = @gppp let\n    f1 = GP(SEKernel())\n    f2 = GP(Matern52Kernel())\n    f3 = f1 + f2\nend","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"a GPPPInput like","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"x_ = randn(5)\nx = GPPPInput(:f3, x_)","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"applied to f","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"fx = f(x, 0.1)","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"constructs a FiniteGP comprising f3 at x_. GPPPInput(:f2, x_) and GPPPInput(:f1, x_) are the analogues for f1 and f2.","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"If you wish to refer to multiple processes in f at the same time, use a BlockData. For example","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"x = BlockData(GPPPInput(:f2, x_), GPPPInput(:f3, x_))","category":"page"},{"location":"input_types/","page":"Input Types","title":"Input Types","text":"would pull out a 10-dimensional FiniteGP, the first 5 dimensions being f2 at x_, the last 5 dimensions being f3 at x_.","category":"page"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Here we document how to do some basic stuff, including learning and inference in kernel parameters, with Stheno.jl. To do this, we that makes use of a variety of packages from the Julia ecosystem. In particular, we'll make use of","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"AdvancedHMC.jl to perform Bayesian inference in our model parameters.\nOptim.jl for point-estimates of our model parameters.\nParameterHandling.jl to make it easy to work with our model's parameters, and to ensure that it plays nicely with Optim and AdvancedHMC.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This guide assumes that you know roughly what's going on conceptually with GPs. If you're new to Gaussian processes, I cannot recommend this video lecture highly enough.","category":"page"},{"location":"getting_started/#Exact-Inference-in-a-GP-in-2-Minutes","page":"Getting Started","title":"Exact Inference in a GP in 2 Minutes","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This is only a slightly more interesting version of the first example on the README. It's slightly more interesting in that we give the kernels some learnable parameters.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# Import the packages we'll need for this bit of the demo.\nusing AbstractGPs\nusing Stheno\n\n# Short length-scale and small variance.\nl1 = 0.4\ns1 = 0.2\n\n# Long length-scale and larger variance.\nl2 = 5.0\ns2 = 1.0\n\n# Specify a GaussianProcessProbabilisticProgramme object, which is itself a GP\n# built from other GPs.\nf = @gppp let\n    f1 = s1 * stretch(GP(Matern52Kernel()), 1 / l1)\n    f2 = s2 * stretch(GP(SEKernel()), 1 / l2)\n    f3 = f1 + f2\nend;\n\n# Generate a sample from f3, one of the processes in f, at some random input locations.\n# Add some iid observation noise, with zero-mean and variance 0.05.\nconst x = GPPPInput(:f3, collect(range(-5.0, 5.0; length=100)));\nσ²_n = 0.05;\nfx = f(x, σ²_n);\nconst y = rand(fx);\n\n# Compute the log marginal likelihood of this observation, just because we can.\nlogpdf(fx, y)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"fx should be thought of as \"f at x\", and is just as a multivariate Normal distribution, with zero mean and covariance matrix","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"cov(f, x) + σ²_n * I","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"As such samples can be drawn from it, and the log probability any particular value under it can be computed, in the same way that you would an MvNormal from Distributions.jl.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can visualise x and y with Plots.jl","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Plots\nplt = plot();\nscatter!(plt, x.x, y; color=:red, label=\"\");\ndisplay(plt);","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"(Image: img)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"It's straightforward to compute the posterior over f:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"f_posterior = posterior(fx, y);","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"f_posterior is another GP, the posterior over f given noisy observations y at inputs x.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Our plotting recipes aren't quite sophisticated enough at the minute to handle GPPPs properly, but plotting still isn't too much work:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"x_plot = range(-7.0, 7.0; length=1000);\nxp = GPPPInput(:f3, x_plot);\nms = marginals(f_posterior(xp));\nplot!(\n    plt, x_plot, mean.(ms);\n    ribbon=3std.(ms), label=\"\", color=:blue, fillalpha=0.2, linewidth=2,\n);\nplot!(\n    plt, x_plot, rand(f_posterior(xp), 10);\n    alpha=0.3, label=\"\", color=:blue,\n);\ndisplay(plt);","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"(Image: img)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"So you've built a simple GP probabilistic programme, performed inference in it, and looked at the posterior. We've only looked at one component of it though – we could look at others. Consider f2:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"xp2 = GPPPInput(:f2, x_plot);\nms = marginals(f_posterior(xp2));\nplot!(\n    plt, x_plot, mean.(ms);\n    ribbon=3std.(ms), label=\"\", color=:red, fillalpha=0.2, linewidth=2,\n);\nplot!(\n    plt, x_plot, rand(f_posterior(xp2, 1e-9), 10);\n    alpha=0.3, label=\"\", color=:red,\n);\ndisplay(plt);","category":"page"},{"location":"getting_started/#Fit-a-GP-with-NelderMead-in-2-Minutes","page":"Getting Started","title":"Fit a GP with NelderMead in 2 Minutes","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Stheno.jl is slightly unusual in that it declines to provide a fit or train function. Why is this? In short, because there's really no need – the ecosystem now contains everything that is needed to easily do this yourself. By declining to insist on an interface, Stheno.jl is able to interact with a wide array of tools, that you can use in whichever way you please.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Optim requires that you provide an objective function with a single Vector{<:Real} parameter for most of its optimisers. We'll use ParameterHandling.jl to build one of these in a way that doesn't involve manually writing code to convert between a structured, human-readable, representation of our parameters (in a NamedTuple) and a Vector{Float64}.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"First, we'll put the model from before into a function:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"function build_model(θ::NamedTuple)\n    return @gppp let\n        f1 = θ.s1 * stretch(GP(SEKernel()), 1 / θ.l1)\n        f2 = θ.s2 * stretch(GP(SEKernel()), 1 / θ.l2)\n        f3 = f1 + f2\n    end\nend","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We've assumed that the parameters will be provided as a NamedTuple, so let's build one and check that the model can be constructed:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using ParameterHandling\n\nθ = (\n    # Short length-scale and small variance.\n    l1 = positive(0.4),\n    s1 = positive(0.2),\n\n    # Long length-scale and larger variance.\n    l2 = positive(5.0),\n    s2 = positive(1.0),\n\n    # Observation noise variance -- we'll be learning this as well.\n    s_noise = positive(0.1),\n)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We've used ParameterHandling.jls positive constraint to ensure that all of the parameters remain positive during optimisation. Note that there's no magic here, and Optim knows nothing about positive. Rather, ParameterHandling knows how to make sure that Optim will optimise the log of the parameters which we want to be positive.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can make this happen with the following:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using ParameterHandling\nusing ParameterHandling: value, flatten\n\nθ_flat_init, unflatten = flatten(θ);\n\n# Concrete types used for clarity only.\nunpack(θ_flat::Vector{Float64}) = value(unflatten(θ_flat))","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Note that θ_flat_init is a Vector{Float64}, which is usable within Optim. unflatten takes it, and reconstructs θ. Moreover, if we ask for value(θ), we'll get a version of θ with all of the positive stuff stripped out, and just leave the NamedTuple. So unpack takes the flat form of the parameters, and converts them into the form that build_model is expecting:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"build_model(unpack(θ_flat_init))","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can now easily define a function which accepts the flat form of the parameters, and return the negative log marginal likelihood of the parameters:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"# nlml = negative log marginal likelihood (of θ)\nfunction nlml(θ_flat)\n    θ = unpack(θ_flat)\n    f = build_model(θ)\n    return -logpdf(f(x, θ.s_noise), y)\nend","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can use any gradient-free optimisation technique from Optim.jl to find the parameters whose negative log marginal likelihood is minimal:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Optim\nresults = Optim.optimize(nlml, θ_flat_init + randn(5), NelderMead())\nθ_opt = unpack(results.minimizer);","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Note that we just added some noise to the initial values to make the optimisation more interesting.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can now use this to construct the posterior GP and look at the posterior in comparison to the true posterior with the known hyperparameters","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"f_opt = build_model(θ_opt);\nf_posterior_opt = posterior(f_opt(x, θ_opt.s_noise), y);\nms_opt = marginals(f_posterior_opt(xp));\nplot!(\n    plt, x_plot, mean.(ms_opt);\n    ribbon=3std.(ms_opt), label=\"\", color=:green, fillalpha=0.2, linewidth=2,\n);\nplot!(\n    plt, x_plot, rand(f_posterior_opt(xp, 1e-9), 10);\n    alpha=0.3, label=\"\", color=:green,\n);\ndisplay(plt);","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"(Image: img)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"(Of course the exact posterior has not been recovered because the exact hyperparameters cannot be expected to be recovered given a finite amount of data over a finite width window.)","category":"page"},{"location":"getting_started/#Fit-a-GP-with-BFGS-in-2-minutes","page":"Getting Started","title":"Fit a GP with BFGS in 2 minutes","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The BFGS algorithm is generally the preferred choice when optimising the hyperparameters of fairly simple GPs. It requires access to the gradient of our nlml function, which can be straightforwardly obtained via reverse-mode algorithmic differentiation, which is provided by Zygote.jl:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using Zygote: gradient\n\n# This will probably take a while to get going as Zygote needs to compile.\nresults = Optim.optimize(\n    nlml,\n    θ->gradient(nlml, θ)[1],\n    θ_flat_init + randn(5),\n    BFGS(),\n    Optim.Options(\n        show_trace=true,\n    );\n    inplace=false,\n)\nθ_bfgs = unpack(results.minimizer);","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Once more visualising the results:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"f_bfgs = build_model(θ_bfgs);\nf_posterior_bfgs = posterior(f_bfgs(x, θ_bfgs.s_noise), y);\nms_bfgs = marginals(f_posterior_bfgs(xp));\nplot!(\n    plt, x_plot, mean.(ms_bfgs);\n    ribbon=3std.(ms_bfgs), label=\"\", color=:orange, fillalpha=0.2, linewidth=2,\n);\nplot!(\n    plt, x_plot, rand(f_posterior_bfgs(xp, 1e-9), 10);\n    alpha=0.3, label=\"\", color=:orange,\n);\ndisplay(plt);","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"(Image: img)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Notice that the two optimisers produce (almost) indistinguishable results.","category":"page"},{"location":"getting_started/#Inference-with-NUTS-in-2-minutes","page":"Getting Started","title":"Inference with NUTS in 2 minutes","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"AdvancedHMC.jl provides a state-of-the-art implementation of the No-U-Turns sampler, which we can use to perform approximate Bayesian inference in the hyperparameters of the GP. This is slightly longer than the previous examples, but it's all set up associated with AdvancedHMC, which is literally a copy-paste from that package's README:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using AdvancedHMC, Zygote\n\n# Define the log marginal joint density function and its gradient\nℓπ(θ) = -nlml(θ) - 0.5 * sum(abs2, θ)\nfunction ∂ℓπ∂θ(θ)\n    lml, back = Zygote.pullback(ℓπ, θ)\n    ∂θ = first(back(1.0))\n    return lml, ∂θ\nend\n\n# Sampling parameter settings\nn_samples, n_adapts = 500, 20\n\n# Draw a random starting points\nθ0 = randn(5)\n\n# Define metric space, Hamiltonian, sampling method and adaptor\nmetric = DiagEuclideanMetric(5)\nh = Hamiltonian(metric, ℓπ, ∂ℓπ∂θ)\nint = Leapfrog(find_good_eps(h, θ0))\nprop = NUTS{MultinomialTS, GeneralisedNoUTurn}(int)\nadaptor = StanHMCAdaptor(n_adapts, Preconditioner(metric), NesterovDualAveraging(0.8, int.ϵ))\n\n# Perform inference.\nsamples, stats = sample(h, prop, θ0, n_samples, adaptor, n_adapts; progress=true)\n\n# Inspect posterior distribution over hyperparameters.\nhypers = unpack.(samples);\nh_l1 = histogram(getindex.(hypers, :l1); label=\"l1\");\nh_l2 = histogram(getindex.(hypers, :l2); label=\"l2\");\nh_s1 = histogram(getindex.(hypers, :s1); label=\"s1\");\nh_s2 = histogram(getindex.(hypers, :s2); label=\"s2\");\ndisplay(plot(h_l1, h_l2, h_s1, h_s2; layout=(2, 2)));","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"(Image: img)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"As expected, the sampler converges to the posterior distribution quickly. One could combine this code with that from the previous sections to make predictions under the posterior over the hyperparameters.","category":"page"},{"location":"getting_started/#Conclusion","page":"Getting Started","title":"Conclusion","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"So you now know how to fit GPs using Stheno.jl, and to investigate their posterior distributions. It's also straightforward to utilise Stheno.jl inside probabilistic programming frameworks like Soss.jl and Turing.jl (see examples folder).","category":"page"},{"location":"kernel_design/#Kernel-Design","page":"Kernel Design","title":"Kernel Design","text":"","category":"section"},{"location":"kernel_design/","page":"Kernel Design","title":"Kernel Design","text":"Please refer to KernelFunctions.jl.","category":"page"},{"location":"#Stheno.jl","page":"Home","title":"Stheno.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Stheno.jl is a package for probabilistic programming with Gaussian processes.","category":"page"},{"location":"internals/#Interfaces","page":"Internals","title":"Interfaces","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"The primary objects in Stheno are some special subtypes of AbstractGP. There are three primary concrete subtypes of AbstractGP:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"WrappedGP: an atomic Gaussian process given by wrapping an AbstractGP.\nCompositeGP: a Gaussian process composed of other WrappedGPs and CompositeGPs, whose properties are determined recursively from the GPs of which it is composed.\nGaussianProcessProbabilisticProgramme / GPPP: a Gaussian process comprising WrappedGPs and CompositeGPs. This is the primary piece of functionality that users should interact with.","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"This documentation provides the information necessary to understand the internals of Stheno, and to extend it with custom functionality.","category":"page"},{"location":"internals/#AbstractGP","page":"Internals","title":"AbstractGP","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"WrappedGP and CompositeGP implement the AbstractGPs.jl API. Please refer to the AbstractGPs.jl docs for more info on this.","category":"page"},{"location":"internals/#diag-methods","page":"Internals","title":"diag methods","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"It is crucial for pseudo-point methods, and for the computation of marginal statistics at a reasonable scale, to be able to compute the diagonal of a given covariance matrix in linear time in the size of its inputs. This, in turn, necessitates that the diagonal of a given cross-covariance matrix can also be computed efficiently as the evaluation of covariance matrices often rely on the evaluation of cross-covariance matrices. As such, we have the following functions in addition to the AbstractGPs API implemented for WrappedGPs and CompositeGPs:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"Function Brief description\nvar(f, x) diag(cov(f, x))\nvar(f, x, x′) diag(cov(f, x, x′))\nvar(f, f′, x, x′) diag(cov(f, f′, x, x′))","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"The second and third rows of the table only make sense when length(x) == length(x′), of course.","category":"page"},{"location":"internals/#WrappedGP","page":"Internals","title":"WrappedGP","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"We can construct a WrappedGP in the following manner:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"f = wrap(GP(m, k), gpc)\n","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"where m is its MeanFunction, k its Kernel. gpc is a GPC object that handles some book-keeping, and is discussed in more depth below.","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"The AbstractGP interface is implemented for WrappedGPs in terms of the AbstractGP that they wrap. So if you want a particular behaviour, just make sure that the AbstractGP that you wrap has the functionality you want.","category":"page"},{"location":"internals/#Aside:-Example-Kernel-implementation","page":"Internals","title":"Aside: Example Kernel implementation","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"It's straightforward to implement a new kernel yourself: simply following the instructions in KernelFunctions.jl and it'll work fine with GPs in Stheno.jl.","category":"page"},{"location":"internals/#GPC","page":"Internals","title":"GPC","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"This book-keeping object doesn't matter from a user's perspective but, unfortunately, we currently expose it to users. Fortunately, it's straightforward to work with. Say you wish to construct a collection of processes:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"# THIS WON'T WORK\nf = GP(mf, kf)\ng = GP(mg, kg)\nh = f + g\n# THIS WON'T WORK","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"You should write","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"# THIS IS GOOD. PLEASE DO THIS\ngpc = GPC()\nf = wrap(GP(mf, kf), gpc)\ng = wrap(GP(mg, kg), gpc)\nh = f + g\n# THIS IS GOOD. PLEASE DO THIS","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"The rule is simple: when constructing GPs that you plan to make interact later in your program, construct them using the same gpc object. For example, DON'T do the following:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"# THIS IS BAD. PLEASE DON'T DO THIS\nf = wrap(GP(mf, kf), GPC())\ng = wrap(GP(mg, kg), GPC())\nh = f + g\n# THIS IS BAD. PLEASE DON'T DO THIS","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"The mistake here is to construct a separate GPC object for each GP. Hopefully, the code errors, but might yield incorrect results.","category":"page"},{"location":"internals/#CompositeGP","page":"Internals","title":"CompositeGP","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"CompositeGPs are constructed as affine transformations of CompositeGPs and GPs. We describe the implemented transformations below.","category":"page"},{"location":"internals/#Addition","page":"Internals","title":"Addition","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"Given AbstractGPs f and g, we define","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"h = f + g","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"to be the CompositeGP sastisfying h(x) = f(x) + g(x) for all x.","category":"page"},{"location":"internals/#Multiplication","page":"Internals","title":"Multiplication","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"Multiplication of AbstractGPs is undefined since the product of two Gaussian random variables is not itself Gaussian. However, we can scale an AbstractGP by either a constant or (deterministic) function.","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"h = c * f\nh = sin * f","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"will both work, and produce the result that h(x) = c * f(x) or h(x) = sin(x) * f(x).","category":"page"},{"location":"internals/#Composition","page":"Internals","title":"Composition","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"h = f ∘ g","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"for some deterministic function g is the composition of f with g. i.e. h(x) = f(g(x)).","category":"page"},{"location":"internals/#cross","page":"Internals","title":"cross","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"h = cross([f, g])","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"for WrappedGPs and CompositeGPs f and g. Think of cross as having stacked f and g together, so that you can work with their joint.","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"For example, if you wanted to sample jointly from f and g at locations x_f and x_g, you could write something like","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"fg = cross([f, g])\nx_fg = BlockData([x_f, x_g])\nrand(fg(x_fg, 1e-6))","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"This particular function isn't part of the user-facing API because it isn't generally needed. It is, however, used extensively in the implementation of the GaussianProcessProbabilisticProgramme.","category":"page"},{"location":"internals/#GPPP","page":"Internals","title":"GPPP","text":"","category":"section"},{"location":"internals/","page":"Internals","title":"Internals","text":"The GaussianProcessProbabilisticProgramme is another AbstractGP which enables provides a thin layer of convenience functionality on top of WrappedGPs and CompositeGPs, and is the primary way in which it is intended that users will interact with this package.","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"A GPPP like this","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"f = @gppp let\n    f1 = GP(SEKernel())\n    f2 = GP(Matern52Kernel())\n    f3 = f1 + f2\nend","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"is equivalent to manually constructing a GPPP using WrappedGPs and CompositeGPs:","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"gpc = GPC()\nf1 = wrap(GP(SEKernel()), gpc)\nf2 = wrap(GP(SEKernel()), gpc)\nf3 = f1 + f2\nf = Stheno.GPPP((f1=f1, f2=f2, f3=f3), gpc)","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"If you take a look at the gaussian_process_probabilistic_programming.jl source, you'll see that it's essentially just the above, and an implementation of the AbstractGPs API on top of a GPPP.","category":"page"},{"location":"internals/","page":"Internals","title":"Internals","text":"A GPPP is a single GP on an extended input domain: (Image: )","category":"page"}]
}
