var documenterSearchIndex = {"docs":
[{"location":"gp_api/#GP-API-1","page":"GP API","title":"GP API","text":"","category":"section"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"This documents the user-facing API as it relates to the GP object.","category":"page"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"This is a more thorough introduction to the internals than the Getting Started guide, which should be refered to if you are new to Stheno.jl. It's somewhere between a reference document and a tutorial.","category":"page"},{"location":"gp_api/#GP-1","page":"GP API","title":"GP","text":"","category":"section"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"The primitive GP type is one of the core components of Stheno.jl. A GP should be thought of as a distribution over real-valued functions, in the same way that a Distributions.Normal is a distribution over real numbers, and Distibutions.MvNormal is a distribution over real-valued vectors.","category":"page"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"GP","category":"page"},{"location":"gp_api/#Stheno.GP","page":"GP API","title":"Stheno.GP","text":"GP{Tm<:MeanFunction, Tk<:Kernel}\n\nA Gaussian Process (GP) with known mean m and kernel k. A book-keeping object gpc is also required, but only matters when composing GPs together.\n\nZero Mean\n\nIf only two arguments are provided, assume the mean to be zero everywhere:\n\njulia> f = GP(Matern32(), GPC());\n\njulia> x = randn(5);\n\njulia> mean(f(x)) == zeros(5)\ntrue\n\njulia> cov(f(x)) == Stheno.pw(Matern32(), x)\ntrue\n\nConstant Mean\n\nIf a Real is provided as the first argument, assume the mean function is constant with that value\n\njulia> f = GP(5.0, EQ(), GPC());\n\njulia> x = randn(5);\n\njulia> mean(f(x)) == 5.0 .* ones(5)\ntrue\n\njulia> cov(f(x)) == Stheno.pw(EQ(), x)\ntrue\n\nCustom Mean\n\nProvide an arbitrary function to compute the mean:\n\njulia> f = GP(x -> sin(x) + cos(x / 2), RQ(3.2), GPC());\n\njulia> x = randn(5);\n\njulia> mean(f(x)) == sin.(x) .+ cos.(x ./ 2)\ntrue\n\njulia> cov(f(x)) == Stheno.pw(RQ(3.2), x)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#FiniteGP-1","page":"GP API","title":"FiniteGP","text":"","category":"section"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"Once constructed, the correct way to interact with a GP is via a FiniteGP, which is just the multivariate Normal given by considering the GP at only a finite set of inputs.","category":"page"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"f = GP(Matern52(), GPC())\nx = randn(10)\nfx = f(x)","category":"page"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"here fx, to be read as \"f at x\", is a FiniteGP to which the following methods apply:","category":"page"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"mean(::Stheno.FiniteGP)\ncov(::Stheno.FiniteGP)\ncov(::Stheno.FiniteGP, ::Stheno.FiniteGP)\nmarginals(::Stheno.FiniteGP)\nrand(::Stheno.AbstractRNG, ::Stheno.FiniteGP, N::Int)\nlogpdf(::Stheno.FiniteGP, ::AbstractVector{<:Real})\nelbo(::Stheno.FiniteGP, y::AbstractVector{<:Real}, ::Stheno.FiniteGP)","category":"page"},{"location":"gp_api/#Statistics.mean-Tuple{Stheno.FiniteGP}","page":"GP API","title":"Statistics.mean","text":"mean(fx::FiniteGP)\n\nCompute the mean vector of fx.\n\njulia> f = GP(Matern52(), GPC());\n\njulia> x = randn(11);\n\njulia> mean(f(x)) == zeros(11)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"gp_api/#Statistics.cov-Tuple{Stheno.FiniteGP}","page":"GP API","title":"Statistics.cov","text":"cov(f::FiniteGP)\n\nCompute the covariance matrix of fx.\n\nNoise-free observations\n\njulia> f = GP(Matern52(), GPC());\n\njulia> x = randn(11);\n\njulia> # Noise-free\n\njulia> cov(f(x)) == Stheno.pw(Matern52(), x)\ntrue\n\nIsotropic observation noise\n\njulia> cov(f(x, 0.1)) == Stheno.pw(Matern52(), x) + 0.1 * I\ntrue\n\nIndependent anisotropic observation noise\n\njulia> s = rand(11);\n\njulia> cov(f(x, s)) == Stheno.pw(Matern52(), x) + Diagonal(s)\ntrue\n\nCorrelated observation noise\n\njulia> A = randn(11, 11); S = A'A;\n\njulia> cov(f(x, S)) == Stheno.pw(Matern52(), x) + S\ntrue\n\n\n\n\n\n","category":"method"},{"location":"gp_api/#Statistics.cov-Tuple{Stheno.FiniteGP,Stheno.FiniteGP}","page":"GP API","title":"Statistics.cov","text":"cov(fx::FiniteGP, gx::FiniteGP)\n\nCompute the cross-covariance matrix between fx and gx.\n\njulia> f = GP(Matern32(), GPC());\n\njulia> x1 = randn(11);\n\njulia> x2 = randn(13);\n\njulia> cov(f(x1), f(x2)) == pw(Matern32(), x1, x2)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"gp_api/#Stheno.marginals-Tuple{Stheno.FiniteGP}","page":"GP API","title":"Stheno.marginals","text":"marginals(f::FiniteGP)\n\nCompute a vector of Normal distributions representing the marginals of f efficiently. In particular, the off-diagonal elements of cov(f(x)) are never computed.\n\njulia> f = GP(Matern32(), GPC());\n\njulia> x = randn(11);\n\njulia> fs = marginals(f(x));\n\njulia> mean.(fs) == mean(f(x))\ntrue\n\njulia> std.(fs) == sqrt.(diag(cov(f(x))))\ntrue\n\n\n\n\n\n","category":"method"},{"location":"gp_api/#Base.rand-Tuple{Random.AbstractRNG,Stheno.FiniteGP,Int64}","page":"GP API","title":"Base.rand","text":"rand(rng::AbstractRNG, f::FiniteGP, N::Int=1)\n\nObtain N independent samples from the marginals f using rng. Single-sample methods produce a length(f) vector. Multi-sample methods produce a length(f) x N Matrix.\n\njulia> f = GP(Matern32(), GPC());\n\njulia> x = randn(11);\n\njulia> rand(f(x)) isa Vector{Float64}\ntrue\n\njulia> rand(MersenneTwister(123456), f(x)) isa Vector{Float64}\ntrue\n\njulia> rand(f(x), 3) isa Matrix{Float64}\ntrue\n\njulia> rand(MersenneTwister(123456), f(x), 3) isa Matrix{Float64}\ntrue\n\n\n\n\n\n","category":"method"},{"location":"gp_api/#Distributions.logpdf-Tuple{Stheno.FiniteGP,AbstractArray{var\"#s343\",1} where var\"#s343\"<:Real}","page":"GP API","title":"Distributions.logpdf","text":"logpdf(f::FiniteGP, y::AbstractVecOrMat{<:Real})\n\nThe logpdf of y under f if is y isa AbstractVector. logpdf of each column of y if y isa Matrix.\n\njulia> f = GP(Matern32(), GPC());\n\njulia> x = randn(11);\n\njulia> y = rand(f(x));\n\njulia> logpdf(f(x), y) isa Real\ntrue\n\njulia> Y = rand(f(x), 3);\n\njulia> logpdf(f(x), Y) isa AbstractVector{<:Real}\ntrue\n\n\n\n\n\n","category":"method"},{"location":"gp_api/#Stheno.elbo-Tuple{Stheno.FiniteGP,AbstractArray{var\"#s343\",1} where var\"#s343\"<:Real,Stheno.FiniteGP}","page":"GP API","title":"Stheno.elbo","text":"elbo(f::FiniteGP, y::AbstractVector{<:Real}, u::FiniteGP)\n\nThe saturated Titsias Evidence LOwer Bound (ELBO) [1]. y are observations of f, and u are pseudo-points.\n\njulia> f = GP(Matern52(), GPC());\n\njulia> x = randn(1000);\n\njulia> z = range(-5.0, 5.0; length=13);\n\njulia> y = rand(f(x, 0.1));\n\njulia> elbo(f(x, 0.1), y, f(z)) < logpdf(f(x, 0.1), y)\ntrue\n\n[1] - M. K. Titsias. \"Variational learning of inducing variables in sparse Gaussian processes\". In: Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics. 2009.\n\n\n\n\n\n","category":"method"},{"location":"gp_api/#Kernels-1","page":"GP API","title":"Kernels","text":"","category":"section"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"This is an ever-growing list. Implementing another kernel would make an excellent first PR...","category":"page"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"EQ\nMatern12\nExp\nMatern32\nMatern52\nRQ\nCosine\nLinear\nPoly\nGammaExp\nWiener\nWienerVelocity","category":"page"},{"location":"gp_api/#Stheno.EQ","page":"GP API","title":"Stheno.EQ","text":"EQ() <: BaseKernel\n\nThe standardised Exponentiated Quadratic kernel. a.k.a. the Radial Basis Function (RBF), or Squared Exponential kernel.\n\nk(x x^prime) = exp( -frac12  x - x^prime_2^2 )\n\nFor length scales etc see stretch, for variance see *.\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#Stheno.Matern12","page":"GP API","title":"Stheno.Matern12","text":"Matern12 <: BaseKernel\n\nThe standardised Matern-1/2 / Exponential kernel:\n\nk(x x^prime) = exp(-x - x^prime_2)\n\nFor length scales etc see stretch, for variance see *.\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#Stheno.Exp","page":"GP API","title":"Stheno.Exp","text":"Exp <: Kernel\n\nThe standardised Exponential kernel. Equivalent to Matern12.\n\nFor length scales etc see stretch, for variance see *.\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#Stheno.Matern32","page":"GP API","title":"Stheno.Matern32","text":"Matern32 <: BaseKernel\n\nThe standardised Matern kernel with ν = 3 / 2.\n\nFor length scales etc see stretch, for variance see *.\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#Stheno.Matern52","page":"GP API","title":"Stheno.Matern52","text":"Matern52 <: BaseKernel\n\nThe standardised Matern kernel with ν = 5 / 2.\n\nFor length scales etc see stretch, for variance see *.\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#Stheno.RQ","page":"GP API","title":"Stheno.RQ","text":"RQ <: BaseKernel\n\nThe standardised Rational Quadratic, with kurtosis α.\n\nFor length scales etc see stretch, for variance see *.\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#Stheno.Cosine","page":"GP API","title":"Stheno.Cosine","text":"Cosine <: BaseKernel\n\nCosine BaseKernel with period parameter p.\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#Stheno.Linear","page":"GP API","title":"Stheno.Linear","text":"Linear{T<:Real} <: BaseKernel\n\nThe standardised linear kernel / dot-product kernel.\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#Stheno.Poly","page":"GP API","title":"Stheno.Poly","text":"Poly{Tσ<:Real} <: BaseKernel\n\nInhomogeneous Polynomial kernel. Poly(p, σ²) creates a Poly{p} with variance σ², defined as\n\nk(xl, xr) = (dot(xl, xr) + σ²)^p\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#Stheno.GammaExp","page":"GP API","title":"Stheno.GammaExp","text":"GammaExp <: BaseKernel\n\nThe γ-Exponential kernel, 0 < γ ⩽ 2, is given by k(xl, xr) = exp(-||xl - xr||^γ).\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#Stheno.Wiener","page":"GP API","title":"Stheno.Wiener","text":"Wiener <: BaseKernel\n\nThe standardised stationary Wiener-process kernel.\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#Stheno.WienerVelocity","page":"GP API","title":"Stheno.WienerVelocity","text":"WienerVelocity <: BaseKernel\n\nThe standardised WienerVelocity kernel.\n\n\n\n\n\n","category":"type"},{"location":"gp_api/#Transformations-of-Kernels-1","page":"GP API","title":"Transformations of Kernels","text":"","category":"section"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"stretch(::Stheno.Kernel, ::Union{Real, AbstractVecOrMat{<:Real}})\n*(::Real, ::Stheno.Kernel)\n+(::Stheno.Kernel, ::Stheno.Kernel)\n*(::Stheno.Kernel, ::Stheno.Kernel)","category":"page"},{"location":"gp_api/#Base.:*-Tuple{Real,Kernel}","page":"GP API","title":"Base.:*","text":"*(σ²::Real, k::Kernel)\n*(k::Kernel, σ²::Real)\n\nThe right way to choose the variance of a kernel. Specifically, construct a kernel that scales the output of k by σ²:\n\njulia> k = EQ();\n\njulia> x = randn(11);\n\njulia> pw(0.5 * k, x) == 0.5 .* Stheno.pw(k, x)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"gp_api/#Base.:+-Tuple{Kernel,Kernel}","page":"GP API","title":"Base.:+","text":"+(kl::Kernel, kr::Kernel)\n\nConstruct the kernel whose value is given by the sum of those of kl and kr.\n\njulia> kl, kr = EQ(), Matern32();\n\njulia> x = randn(11);\n\njulia> pw(kl + kr, x) == pw(kl, x) + pw(kr, x)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"gp_api/#Base.:*-Tuple{Kernel,Kernel}","page":"GP API","title":"Base.:*","text":"*(kl::Kernel, kr::Kernel)\n\nConstruct the kernel whose value is given by the product of those of kl and kr.\n\njulia> kl, kr = EQ(), Matern32();\n\njulia> x = randn(11);\n\njulia> pw(kl * kr, x) == pw(kl, x) .* pw(kr, x)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"gp_api/#Kernel-Convenience-1","page":"GP API","title":"Kernel Convenience","text":"","category":"section"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"kernel","category":"page"},{"location":"gp_api/#Stheno.kernel","page":"GP API","title":"Stheno.kernel","text":"kernel(k::Kernel;  l::Real=nothing, s::Real=nothing)\n\nConvenience functionality to provide a kernel with a length scale l, and to scale the variance of k by s. Simply applies the stretch and * functions.\n\njulia> k1 = kernel(EQ(); l=1.1, s=0.9);\n\njulia> k2 = 0.9 * stretch(EQ(), 1 / 1.1);\n\njulia> x = randn(11);\n\njulia> pw(k1, x) == pw(k2, x)\ntrue\n\n\n\n\n\n","category":"function"},{"location":"gp_api/#MeanFunctions-1","page":"GP API","title":"MeanFunctions","text":"","category":"section"},{"location":"gp_api/#","page":"GP API","title":"GP API","text":"These are implicit. Please refer to the GP documentation for details.","category":"page"},{"location":"input_types/#Input-Types-1","page":"Input Types","title":"Input Types","text":"","category":"section"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"Stheno enables the user to handle any type of input domain they wish and provides a common API that users must implement when considering a new way of representing input data to ensure that the package knows how to handle them appropriately.","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"Discussed below is this interface's core assumptions, a detailed account of a couple of important concrete input types. Additionally, we provide a worked-example of a new input type.","category":"page"},{"location":"input_types/#The-Central-Assumption-1","page":"Input Types","title":"The Central Assumption","text":"","category":"section"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"The central assumption made in all user-facing and internal functions is this: when a collection of inputs are required, they subtype AbstractVector. For example, x isa AbstractVector when indexing into a GP:","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"f(x, σ²)","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"or computing the covariance matrix associated with a kernel:","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"pw(EQ(), x)","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"When computing the cross-covariance matrix between two GPs","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"cov(f, g, x_f, x_g)","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"x_f and x_g must be AbstractVectors. All other operations involving collections of inputs have the same restrictions. Always AbstractVectors.","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"For example, this means that when handling multi-dimensional inputs stored in a Matrix they must be wrapped so that the package knows to treat them as a vector — more on this in below in D-dimensional Euclidean Spaces.","category":"page"},{"location":"input_types/#Dimensional-Euclidean-Space-1","page":"Input Types","title":"1-Dimensional Euclidean Space","text":"","category":"section"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"When constructing a GP whose domain is the real-line, for example when using a GP to solve some kind of time-series problem, it is sufficient to work with Vector{<:Real}s of inputs. As such, the following is completely valid:","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"using Stheno: GPC\nf = GP(EQ(), GPC())\nx = randn(10)\nf(x)","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"It is also possible to work with other AbstractArrays that represent a vector of 1-dimensional points, e.g.","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"x = range(-5.0, 5.0; length=100)\nf(x)","category":"page"},{"location":"input_types/#D-Dimensional-Euclidean-Space-1","page":"Input Types","title":"D-Dimensional Euclidean Space","text":"","category":"section"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"Many applications of interest involve more than a single input-dimension, such as spatio-temporal modeling or Machine Learning tasks. For these cases, we provide ColVecs <: AbstractVector.","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"X_data = randn(5, 100)\nX = ColVecs(X_data)","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"tells Stheno that it should treat each column of X_data as a vector-valued input. Phrased differently, X is an AbstractVector{T} where T <: Vector{<:Real}, which stores its elements in memory as a dense matrix. This approach has the advantage of making it completely explicit how Stheno treats a matrix of data, and also simplifies quite a bit of the internal machinery, as all vectors of inputs can be assumed to be a subtype of AbstractVector.","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"Future plans include a RowVecs type, which would instead treat each row of X_data as a vector-valued input. If you would like this feature, please raise an issue or a PR to let us know there's a demand for it. The worked example below actually makes some headway on this, so it provides an excellent starting point for a PR!","category":"page"},{"location":"input_types/#Structure-in-D-Dimensional-Euclidean-Space-1","page":"Input Types","title":"Structure in D-Dimensional Euclidean Space","text":"","category":"section"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"Consider a rectilinear grid of points in D-dimensional Euclidean space. Such grids of points can be represented in a more memory-efficient manner than can arbitrarily locate sets of points. Moreover, this structure can be exploited to accelerate inference for certain types of problems dramatically. Other such examples exist e.g., uniform grids in N-dimensions, and can be exploited to more efficiently represent input data and to accelerate inference.","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"Work to exploit these kinds of structures is on-going at the time of writing and will be documented before merging.","category":"page"},{"location":"input_types/#Worked-Example-1","page":"Input Types","title":"Worked Example","text":"","category":"section"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"As discussed, ColVecs is already supported for inputs in D-dimensional Euclidean space, where Matrix stores a collection of inputs, and each column is an input. The following example presents RowVecs, which represents collections inputs residing in D-dimensional Euclidean space, but the interpretation is different: each row of the matrix corresponds to an input.","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"Firstly, the new data structure is specified:","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"struct RowVecs{T<:Real} <: AbstractVector{Vector{T}}\n    X::Matrix{T}\nend","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"Observe that it subtypes AbstractVector, and each element is a Vector{T<:Real}. It has a single field X, which is a matrix of elements. It is necessary to implement some parts of the AbstractArray interface to ensure printing and various consistency checks inside the package work as intended:","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"Base.length(x::RowVecs) = size(x.X, 1)\nBase.size(x::RowVecs) = (length(x),)\nBase.getindex(x::RowVecs, n::Int) = x.X[n, :]","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"This structure prints nicely and pass some consistency checks, but none of the base Kernels in the package know how to treat it. This means that, for example, new pw and ew methods that are specialised to RowVec must be added:","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"import Stheno: pw\nusing Distances: SqEuclidean\npw(k::EQ, x::RowVecs, x′::RowVecs) = exp.(.-pw(SqEuclidean(), x.X, x′.X; dims=1) ./ 2)\n# insert implementations for the unary pw and the two ew methods","category":"page"},{"location":"input_types/#","page":"Input Types","title":"Input Types","text":"In the worst case, this means that every Kernel needs four new methods to handle a new data structure. Fortunately, this case isn't typical. Most of the Kernels in src/kernels.jl are implemented in terms of the SqEuclidean and Euclidean distances, and are generically typed. As such it is sufficient to add new ew and pw methods involving those types, without the need to re-implement those methods for kernels. See src/util/distances.jl for examples.","category":"page"},{"location":"composite_gp_api/#CompositeGP-API-1","page":"CompositeGP API","title":"CompositeGP API","text":"","category":"section"},{"location":"composite_gp_api/#","page":"CompositeGP API","title":"CompositeGP API","text":"WIP. Please come back later :(","category":"page"},{"location":"getting_started/#Getting-Started-1","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Here we document how to achieve the basic things that any GP package aught to be able to do. We lean heavily on the rest of the Julia ecosystem for each of these examples – this page really exemplifies the way in which different packages play together nicely in the Julia!","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"This guide assumes that you know roughly what's going on conceptually with GPs. If you're new to Gaussian processes, I cannot recommend this video lecture highly enough.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"We shall first cover the most low-level ways to perform inference in both the process and hyperparameters, and then discuss how to integrate Stheno models with Soss.jl to remove most of the annoying boilerplate code.","category":"page"},{"location":"getting_started/#Exact-Inference-in-a-GP-in-2-Minutes-1","page":"Getting Started","title":"Exact Inference in a GP in 2 Minutes","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"While Stheno offers some bells and whistles that other GP frameworks do not, it also offers the same functionality as a usual GP framework.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"using Stheno\n\n# Choose the length-scale and variance of the process.\nl = 0.4\nσ² = 1.3\n\n# Construct a kernel with this variance and length scale.\nk = σ² * stretch(Matern52(), 1 / l)\n\n# Specify a zero-mean GP with this kernel. Don't worry about the GPC object.\nf = GP(k, GPC())\n\n# Generate a sample from this GP at some random input locations.\n# Add some iid observation noise, with zero-mean and variance 0.05.\nconst x = randn(100)\nσ²_n = 0.05\nfx = f(x, σ²_n)\nconst y = rand(fx)\n\n# Compute the log marginal likelihood of this observation, just because we can.\nlogpdf(fx, y)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"fx should be thought of as \"f at x\", and is just as a multivariate Normal distribution, with zero mean and covariance matrix","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"pw(k, x) + σ²_n * I","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"As such samples can be drawn from it, and the log probability any particular value under it can be computed, in the same way that you would an MvNormal from Distributions.jl.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"We can visualise x and y with Plots.jl","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"using Plots\nplt = plot();\nscatter!(plt, x, y; color=:red, label=\"\");\ndisplay(plt);","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"(Image: img)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"It's straightforward to compute the posterior over f:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"f_posterior = f | Obs(fx, y)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"f_posterior is another GP, the posterior over f given noisy observations y at inputs x. Equivalently:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"f_posterior = f | (fx ← y) # ← is \\leftarrow[TAB]","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"This is just syntactic sugar for the above. You can use it, or not, the choice is entirely your own.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Stheno.jl knows how to use Plots.jl to plot GPs, so it's straightforward to look at the posterior:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"x_plot = range(-4.0, 4.0; length=1000);\nplot!(plt, f_posterior(x_plot); samples=10, label=\"\", color=:blue);\ndisplay(plt);","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"(Image: img)","category":"page"},{"location":"getting_started/#Fit-a-GP-with-NelderMead-in-2-Minutes-1","page":"Getting Started","title":"Fit a GP with NelderMead in 2 Minutes","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Stheno.jl is slightly unusual in that it declines to provide a fit or train function. Why is this? In short, because it's hard to design a one-size-fits-all interface for training a GP that composes well with the rest of the tools in the Julia ecosystem, and you really want to avoid creating any impediments to interacting with other tools in the ecosystem.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Here we demonstrate the simplest most low-level way to work with Stheno, in which everything is done manually. This example is to demonstrate that the previous section provides all of the basic building blocks that you need to solve regression problems with GPs.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"function unpack(θ)\n    σ² = exp(θ[1]) + 1e-6\n    l = exp(θ[2]) + 1e-6\n    σ²_n = exp(θ[3]) + 1e-6\n    return σ², l, σ²_n\nend\n\n# nlml = negative log marginal likelihood (of θ)\nfunction nlml(θ)\n    σ², l, σ²_n = unpack(θ)\n    k = σ² * stretch(Matern52(), 1 / l)\n    f = GP(k, GPC())\n    return -logpdf(f(x, σ²_n), y)\nend","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Hopefully it's clear what we mean by low-level here. We've manually defined a function to unpack a parameter vector θ and use this to construct a function that computes the negative log marginal likelihood of θ. We can use any gradient-free optimisation technique from Optim.jl to find the parameters whose negative log marginal likelihood is minimal:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"using Optim\nθ0 = randn(3);\nresults = Optim.optimize(nlml, θ0, NelderMead())\nσ²_ml, l_ml, σ²_n_ml = unpack(results.minimizer);","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"We can now use this to construct the posterior GP and look at the posterior in comparison to the true posterior with the known hyperparameters","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"k = σ²_ml * stretch(Matern52(), 1 / l_ml);\nf = GP(k, GPC());\nf_posterior_ml = f | Obs(f(x, σ²_n_ml), y);\nplot!(plt, f_posterior_ml(x_plot); samples=10, color=:green, label=\"\");\ndisplay(plt);","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"(Image: img)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"(Of course the exact posterior has not been recovered because the exact hyperparameters cannot be expected to be recovered.)","category":"page"},{"location":"getting_started/#Fit-a-GP-with-BFGS-in-2-minutes-1","page":"Getting Started","title":"Fit a GP with BFGS in 2 minutes","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"The BFGS algorithm is generally the preferred choice when optimising the hyperparameters of fairly simple GPs. It requires access to the gradient of our nlml function, which can be straightforwardly obtained via reverse-mode algorithmic differentiation, which is provided by Zygote.jl:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"using Zygote: gradient\nθ0 = randn(3);\nresults = Optim.optimize(nlml, θ->gradient(nlml, θ)[1], θ0, BFGS(); inplace=false)\nσ²_bfgs, l_bfgs, σ²_n_bfgs = unpack(results.minimizer);","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Once more visualising the results:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"k = σ²_bfgs * stretch(Matern52(), 1 / l_bfgs);\nf = GP(k, GPC());\nf_posterior_bfgs = f | Obs(f(x, σ²_n_bfgs), y);\nplot!(plt, f_posterior_bfgs(x_plot); samples=10, color=:purple, label=\"\");\ndisplay(plt);","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"(Image: img)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Notice that the two optimisers produce (almost) indistinguishable results.","category":"page"},{"location":"getting_started/#Inference-with-NUTS-in-2-minutes-1","page":"Getting Started","title":"Inference with NUTS in 2 minutes","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"AdvancedHMC.jl provides a state-of-the-art implementation of the No-U-Turns sampler, which we can use to perform approximate Bayesian inference in the hyperparameters of the GP. This is slightly longer than the previous examples, but it's all set up associated with AdvancedHMC, which is literally a copy-paste from that package's README:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"using AdvancedHMC, Zygote\n\n# Define the log marginal likelihood function and its gradient\nℓπ(θ) = -nlml(θ)\nfunction ∂ℓπ∂θ(θ)\n    lml, back = Zygote.pullback(ℓπ, θ)\n    ∂θ = first(back(1.0))\n    return lml, ∂θ\nend\n\n# Sampling parameter settings\nn_samples, n_adapts = 100, 20\n\n# Draw a random starting points\nθ0 = randn(3)\n\n# Define metric space, Hamiltonian, sampling method and adaptor\nmetric = DiagEuclideanMetric(3)\nh = Hamiltonian(metric, ℓπ, ∂ℓπ∂θ)\nint = Leapfrog(find_good_eps(h, θ0))\nprop = NUTS{MultinomialTS, GeneralisedNoUTurn}(int)\nadaptor = StanHMCAdaptor(n_adapts, Preconditioner(metric), NesterovDualAveraging(0.8, int.ϵ))\n\n# Perform inference\nsamples, stats = sample(h, prop, θ0, n_samples, adaptor, n_adapts; progress=true)\n\n# Inspect posterior distribution over hyperparameters.\nhypers = unpack.(samples);\nplt_hypers = plot();\nplot!(plt_hypers, getindex.(hypers, 1); label=\"variance\");\nplot!(plt_hypers, getindex.(hypers, 2); label=\"length scale\");\nplot!(plt_hypers, getindex.(hypers, 3); label=\"obs noise variance\");\ndisplay(plt_hypers);","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"(Image: img)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"As expected, the sampler converges to the posterior distribution quickly. One could combine this code with that from the previous sections to make predictions under the posterior over the hyperparameters.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Also note that we didn't specify a prior over the kernel parameters in this example, so essentially used an improper prior. We could have used a proper prior by appropriately modifying ℓπ.","category":"page"},{"location":"getting_started/#Automation-with-Soss.jl-1","page":"Getting Started","title":"Automation with Soss.jl","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"In all of the examples above it has been necessary to define the function unpack to convert between a vector representation of our hyperparameters and one that is useful for computing the nlml. Moreover, it has been necessary to manually define conversions between unconstrained and constrained parametrisations of our hyperparameters. While this is fairly straightforward for a small number of parameters, it's clearly not a scalable solution as models grow in size and complexity.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"Instead we can make use of functionality defined in Soss.jl to","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"specify priors for the model parameters\nautomatically derive transformations between a vector of unconstrained real numbers and a form that Soss models know how to handle","category":"page"},{"location":"getting_started/#Specifying-a-Soss-model-1","page":"Getting Started","title":"Specifying a Soss model","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"using Soss\n\nM = @model x begin\n    σ² ~ LogNormal(0, 1)\n    l ~ LogNormal(0, 1)\n    σ²_n ~ LogNormal(0, 1)\n    f = Stheno.GP(σ² * stretch(Matern52(), 1 / l), Stheno.GPC())\n    y ~ f(x, σ²_n + 1e-3)\nend","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"The above defines the model M, with LogNormal(0, 1) priors over each of our hyperparameters.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"const t = xform(M(x=x), (y=y,))","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"t is a function that transforms between a vector of model parameters and a NamedTuple that Soss understands.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"function ℓπ(θ)\n    (θ_, logjac) = Soss.transform_and_logjac(t, θ)\n    return logpdf(M(x=x), merge(θ_, (y=y,))) + logjac\nend\nnlj(θ) = -ℓπ(θ)","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"ℓπ computes the log joint of the hyperparameters and data. Note that we have replaced the unpacking and transformation functionality from the previous code with the automatically generated function t.","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"The examples that follow simply repeat those found above, but utilise t.","category":"page"},{"location":"getting_started/#Fit-with-Nelder-Mead:-Stheno-Soss-Optim-1","page":"Getting Started","title":"Fit with Nelder Mead: Stheno + Soss + Optim","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"θ0 = randn(3);\nresults = Optim.optimize(nlj, θ0, NelderMead());\nθ, _ = Soss.transform_and_logjac(t, results.minimizer);","category":"page"},{"location":"getting_started/#Fit-with-BFGS:-Stheno-Soss-Zygote-Optim-1","page":"Getting Started","title":"Fit with BFGS: Stheno + Soss + Zygote + Optim","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"# Hack to make Zygote play nicely with a particular thing in Distributions.\nZygote.@nograd Distributions.insupport\n\n# Point estimate of parameters via BFGS.\nθ0 = randn(3);\nresults = Optim.optimize(nlj, θ->first(gradient(nlj, θ)), θ0, BFGS(); inplace=false)\nθ, _ = Soss.transform_and_logjac(t, results.minimizer);","category":"page"},{"location":"getting_started/#Inference-with-NUTS:-Stheno-Soss-Zygote-AdvancedHMC-1","page":"Getting Started","title":"Inference with NUTS: Stheno + Soss + Zygote + AdvancedHMC","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"# Inference with AdvancedHMC.\nfunction ∂ℓπ∂θ(θ)\n    lml, back = Zygote.pullback(ℓπ, θ)\n    ∂θ = first(back(1.0))\n    return lml, ∂θ\nend\n\n# Sampling parameter settings\nn_samples, n_adapts = 100, 20\n\n# Draw a random starting points\nθ0 = randn(3)\n\n# Define metric space, Hamiltonian, sampling method and adaptor\nmetric = DiagEuclideanMetric(3)\nh = Hamiltonian(metric, ℓπ, ∂ℓπ∂θ)\nint = Leapfrog(find_good_eps(h, θ0))\nprop = NUTS{MultinomialTS, GeneralisedNoUTurn}(int)\nadaptor = StanHMCAdaptor(n_adapts, Preconditioner(metric), NesterovDualAveraging(0.8, int.ϵ))\n\n# Perform inference using NUTS.\nsamples, stats = sample(h, prop, θ0, n_samples, adaptor, n_adapts; progress=true)\n\nhypers = first.(Soss.transform_and_logjac.(Ref(t), samples));","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"hypers could be plotted in exactly the same manner as before.","category":"page"},{"location":"getting_started/#Conclusion-1","page":"Getting Started","title":"Conclusion","text":"","category":"section"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"That's it! You now know how to do typical GP stuff in Stheno. In particular how to:","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"specify a kernel with a particular length-scale and variance\nconstruct a GP\nsample from a GP, and specify an observation noise\ncompute the log marginal likelihood of some observations\nvisualise a simple 1D example\ninfer kernel parameters in a variety of ways\nutilise Soss.jl to automatically handle parameters and their transformations","category":"page"},{"location":"getting_started/#","page":"Getting Started","title":"Getting Started","text":"This are just the basic features of Stheno, that you could expect to find in any other GP package. We haven't covered any of the fancy features of Stheno yet though.","category":"page"},{"location":"kernel_design/#Kernel-Design-1","page":"Kernel Design","title":"Kernel Design","text":"","category":"section"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"Stheno.jl provides a compositional approach to providing all of the usual niceties for kernel construction. We outline the basic kernels, how you can choose their length scales / variances, and the ways in which you can combine them.","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"The interface to use the kernels to compute things isn't strictly user-facing, so please refer to the Internals section on kernels for more info about doing things with a kernel once you've built it.","category":"page"},{"location":"kernel_design/#Base-Kernels-1","page":"Kernel Design","title":"Base Kernels","text":"","category":"section"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"Stheno maintains a diverse collection of so-called \"base\" kernels. These include, but are not limited to, the","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"exponentiated quadratic a.k.a. squared exponential, radial basis function (RBF)\nvarious matern kernels\nrational quadratic","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"All of these base kernels have unit length scale and variance by default. We'll see shortly how to move beyond this.","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"The collection of base kernels can be found in src/GP/kernels.jl – at any given point in time it should be considered the definitive source of knowledge regarding the available base kernels. Their implementations are simple, so it's highly recommended to take a look to see what's available.","category":"page"},{"location":"kernel_design/#Length-scales-and-variances-1","page":"Kernel Design","title":"Length scales and variances","text":"","category":"section"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"As discussed, the base kernels don't come equipped with any notion of a length scale of variance. It's simple to provide any of them with such properties though. The approach we take here has the distinct benefits of immediately applying to any new kernels that are implemented, so if you decide to implement a new kernel there's no need to worry about its length scale or variance, just implement it assume they're both 1 and you'll be grand. This is great, as you get ARD and factor-analyis kernels for free when you implement a new kernel!","category":"page"},{"location":"kernel_design/#Kernel-Variance-/-Amplitude-1","page":"Kernel Design","title":"Kernel Variance / Amplitude","text":"","category":"section"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"Given a kernel k, and a desired variance s, we have that","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"(s * k)(x, y) = s * k(x, y)","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"In other words, s * k returns a new kernel that's the original kernel with a scaled variance.","category":"page"},{"location":"kernel_design/#Length-Scale-1","page":"Kernel Design","title":"Length Scale","text":"","category":"section"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"Given a kernel k, and a desired inverse length scale a, we have that","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"stretch(k, a)(x, y) = k(a * x, a * y)","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"which, if you do the maths, you'll see is exactly what you needed to do to implement the length scale of a kernel.","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"This extends to ARD kernels straightforwardly – just make a an AbstractVector{<:Real}. Furthermore it extends to matrices, enabling learning of a low-dimensional representation of the data.","category":"page"},{"location":"kernel_design/#Kernel-Composition-1","page":"Kernel Design","title":"Kernel Composition","text":"","category":"section"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"The core piece of functionality that makes Stheno.jl's kernels work is composition. For example, the Sum kernel lets you write","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"k3 = k1 + k2","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"and the Product kernel enables","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"k3 = k1 * k2","category":"page"},{"location":"kernel_design/#","page":"Kernel Design","title":"Kernel Design","text":"In fact, the variance and stretch functionality discussed above is implemented in terms of composite kernels. Take a look towards the bottom of the src/GP/kernels.jl file if you're interested how things. The Sum and Product kernels are good places to start here.","category":"page"},{"location":"#Stheno.jl-1","page":"Home","title":"Stheno.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Stheno.jl is a package for probabilistic programming with Gaussian processes.","category":"page"},{"location":"internals/#Interfaces-1","page":"Internals","title":"Interfaces","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The primary objects in Stheno are AbstractGPs, which represent Gaussian processes. There are two primary concrete subtypes of AbstractGP:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"GP: an atomic Gaussian process, whose MeanFunction and Kernel are specified directly.\nCompositeGP: a Gaussian process composed of other AbstractGPs, whose properties are determined recursively from the AbstractGPs of which it is composed.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"This documentation provides the information necessary to understand the internals of Stheno, and to extend it with custom functionality.","category":"page"},{"location":"internals/#AbstractGP-1","page":"Internals","title":"AbstractGP","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The AbstractGP interface enables one to compute quantities required when working with Gaussian processes in practice, namely to compute their logpdf and sample from them at particular locations in their domain.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Function Brief description\nmean_vector(f, x) The mean vector of f at inputs x\ncov(f, x) covariance matrix of f at inputs x\ncov(f, x, x′) covariance matrix between f at x and x′\ncov(f, f′, x, x′) cross-covariance matrix between f at x and f′ at x′","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"It should always hold that cov(f, x) ≈ cov(f, f, x, x), but in some critical cases cov(f, x) is significantly faster.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"GP and CompositeGP are concrete subtypes of AbstractGP, and can be found here and here respectively.","category":"page"},{"location":"internals/#diag-methods-1","page":"Internals","title":"diag methods","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"It is crucial for pseudo-point methods, and for the computation of marginal statistics at a reasonable scale, to be able to compute the diagonal of a given covariance matrix in linear time in the size of its inputs. This, in turn, necessitates that the diagonal of a given cross-covariance matrix can also be computed efficiently as the evaluation of covariance matrices often rely on the evaluation of cross-covariance matrices. As such, we have the following functions:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Function Brief description\ncov_diag(f, x) diag(cov(f, x))\ncov_diag(f, x, x′) diag(cov(f, x, x′))\ncov_diag(f, f′, x, x′) diag(cov(f, f′, x, x′))","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The second and third rows of the table only make sense when length(x) == length(x′), of course.","category":"page"},{"location":"internals/#GP-1","page":"Internals","title":"GP","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"We can construct a GP in the following manner:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"GP(m, k, gpc)","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"where m is its MeanFunction, k its Kernel. gpc is a GPC object that handles some book-keeping, and is discussed in more depth later (don't worry it's very straightforward, and only mildly annoying).","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The AbstractGP interface is implemented for GPs via operations on their MeanFunction and Kernel. It is therefore straightforward to extend the range of functionality offered by Stheno.jl by simply implementing a new MeanFunction or Kernel that satisfies their interface, which we detail below.","category":"page"},{"location":"internals/#MeanFunctions-1","page":"Internals","title":"MeanFunctions","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"MeanFunctions are unary functions with Real-valued outputs with a single-method interface. They must implement elementwise (aliased to ew for brevity) with the signature","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"ew(m::MyMeanFunction, x::AbstractVector)","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"This function applies the MeanFunction to each element of x, and should return an AbstractVector{<:Real} of the same length as x. Note that x represents a vector of observations, not a single feature vector. Some example implementations can be found here.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Note that while MeanFunctions are in principle functions, their interface does not require that we can evaluate m(x[p]), only that the \"vectorised\" elementwise function be implemented. This is due to the fact that, in practice, we only ever need the result of elementwise.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"There are a couple of methods of GP which are specialised to particular MeanFunctions:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"GP(k::Kernel, gpc::GPC) == GP(ZeroMean(), k, gpc)\nGP(c::Real, k::Kernel, gpc::GPC) == GP(ConstMean(c), k, gpc)","category":"page"},{"location":"internals/#Kernels-1","page":"Internals","title":"Kernels","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"A Kernel is a binary function, returning a Real-valued result. Kernels are only slightly more complicated than MeanFunctions, having a four-method interface:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"# Binary methods\new(k::MyKernel, x::AbstractVector, x′::AbstractVector) # \"Binary elementwise\"\npw(k::MyKernel, x::AbstractVector, x′::AbstractVector) # \"Binary pairwise\"\n\n# Unary methods\new(k::MyKernel, x::AbstractVector) # \"Unary elementwise\"\npw(k::MyKernel, x::AbstractVector) # \"Unary pairwise\"","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Again, ew === elementwise and pw === pairwise.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Note that, as with MeanFunctions, the Kernel interface does not require that one can actually evaluate k(x[p], x′[q]), as in practice this functionality is never really required and would otherwise be extra code to maintain.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"We consider each method in turn.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Binary elementwise: compute k(x[p], x′[p]) for p in eachindex(x). x and x′ are assumed to be of the same length. Returns a subtype of AbstractVector{<:Real}, of the same length as x and x′.\nBinary pairwise: compute k(x[p], x′[q]) for p in eachindex(x) and q in eachindex(x′). x and x′ need not be of the same length. Returns a subtype of AbstractMatrix{<:Real} whose size is (length(x), length(x′)).\nUnary elementwise: compute k(x[p], x[p]) for p in eachindex(x). Returns a subtype of AbstractVector{<:Real} of the same length as x.\nUnary pairwise: compute k(x[p], x[q]) for p in eachindex(x) and q in eachindex(x). Returns a subtype of AbstractMatrix{<:Real} whose size is (length(x), length(x)). Crucially, output must be positive definite and (approximately) symmetric.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Example implementations can be found below. Often you'll find that multiple versions of each method are implemented, specialised to different input types. For example, the EQ kernel has (at the time of writing) two implementations of each method, one for inputs AbstractVector{<:Real}, and one for ColVecs <: AbstractVector inputs. These specialisations are for performance purposes.","category":"page"},{"location":"internals/#Example-Kernel-implementation-1","page":"Internals","title":"Example Kernel implementation","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"It's straightforward to implement a new kernel yourself: define a new type and implement the two pw and ew methods required to make it play nicely with everything else in Stheno. This process is broken down below.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"using Stheno\nusing Stheno: Kernel\n\nstruct EQ{Tl<:Real} <: Kernel\n    l::Tl\nend\n\n_eq(l::Real, xl::Real, xr::Real) = exp(-((xl - xr) / l)^2)","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The above defines a structure that represents an Exponentiated Quadratic (a.k.a. RBF / Radial Basis Function, Squared Exponential) kernel with length scale l. The _eq function defines how the kernel operates on a pair of real values given the length-scale and is just a helper function used to define the pw and ew methods below.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"import Stheno: pw\npw(k::EQ, x::AbstractVector{<:Real}) = _eq.(k.l, x, x')","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"This is one way to implement the unary pairwise method for this kernel using Julia's broadcasting functionality. Sampling from the prior and computing the log marginal probability of data is possible given just this method:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"# Construct a GP. See below for info regarding the GPC, but don't worry\n# too much about it.\nusing Stheno: GPC\nl = 2.4\nf = GP(EQ(l), GPC())\n\n# Sample from the prior and add some iid observation noise with variance 0.1.\nx = range(-5.0, 5.0; length=100)\ny = rand(f(x, 0.1))\n\n# Compute the log marginal probability of `y`.\nlogpdf(f(x, 0.1), y)","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"To compute posterior predictive likelihoods, and to sample from the posterior, the binary pairwise method is required:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"# Define binary `pairwise` method.\npw(k::EQ, xl::AbstractVector{<:Real}, xr::AbstractVector{<:Real}) = _eq.(k.l, xl, xr')\n\n# Compute posterior process.\nf_post = f | (f(x, 0.1) ← y);\n\n# Sample from posterior predictive with a tiny amount of noise for numerical stability.\nx_pr = randn(10)\ny_pr = rand(f_post(x_pr, 1e-12))\n\n# Compute the log marginal conditional probability of the posterior sample.\nlogpdf(f_post(x_pr, 1e-12), y_pr)","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Often the marginal statistics of a GP are helpful, and it is for these (and pseudo-point methods) that the elementwise / ew methods are required:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"import Stheno: ew\new(k::EQ, xl::AbstractVector{<:Real}, xr::AbstractVector{<:Real}) = _eq.(k.l, xl, xr)\new(k::EQ, x::AbstractVector{<:Real}) = ones(length(x))","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"It is now possible to compute the posterior marginal statistics at a large number of points efficiently:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"f_post_marginals = marginals(f_post(range(-10.0, 10.0; length=5_000)))\nmeans = mean.(f_post_marginals)\nstds = std.(f_post_marginals)","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"and to use Stheno's plotting functionality for pretty-printing:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"using Plots # Possibly type `]add Plots`\nplot(f_post(range(-10.0, 10.0; length=2_000)); samples=10, color=:blue, label=\"\")\nscatter!(x, y; markersize=2.0, color=:black, label=\"\")","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Stheno provides a more general implementation of the Exponentiated Quadratic (EQ) kernel, which is only a minor extension of the above, and can be found in kernels.jl alongside various other kernels that are available.","category":"page"},{"location":"internals/#Why-no-sensible-fallbacks?-1","page":"Internals","title":"Why no sensible fallbacks?","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Early versions of Stheno required that new kernels also define a method that evaluates the kernel at a pair of inputs (i.e., the functionality provided by the _eq function in the example above). This requirement was found to be of minimal use in practice in any situation other than prototyping as, in the vast majority of cases, better performance is obtained by directly implementing pw and ew. This difference in performance is often sufficiently stark as to render them useless for most practical purposes, meaning that it usually better for Stheno to error and let the user know that an efficient method is missing than to proceed with the fallback.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"If you feel this approach is helpful for your work, it is recommended to adapt the code developed in the example above and define a function similar to _ew that suits your own needs. Alternatively, if you would really to see this functionality, please raise an issue or open a PR. As with most things in the Julia ecosystem, someone will build it if there is sufficient demand.","category":"page"},{"location":"internals/#AbstractGP-Interface-Implementation-1","page":"Internals","title":"AbstractGP Interface Implementation","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Given the above, the AbstractGP interface is straightforward to implement for GPsme, as each method of mean_vector and cov can be implemented in terms of ew and pw. See here for the implementation.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"If you are interested just in working with a single GP object, with a known MeanFunction and Kernel, this is probably as far as you need to go. Simply implement you own fancy Mean and Kernel objects, or approximations to them, and have some fun / do some research.","category":"page"},{"location":"internals/#CompositeGP-1","page":"Internals","title":"CompositeGP","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"CompositeGPs are constructed as affine transformations of CompositeGPs and GPs. We describe the implemented transformations below.","category":"page"},{"location":"internals/#Addition-1","page":"Internals","title":"Addition","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Given AbstractGPs f and g, we define","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"h = f + g","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"to be the CompositeGP sastisfying h(x) = f(x) + g(x) for all x.","category":"page"},{"location":"internals/#Multiplication-1","page":"Internals","title":"Multiplication","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Multiplication of AbstractGPs is undefined since the product of two Gaussian random variables is not itself Gaussian. However, we can scale an AbstractGP by either a constant or (deterministic) function.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"h = c * f\nh = sin * f","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"will both work, and produce the result that h(x) = c * f(x) or h(x) = sin(x) * f(x).","category":"page"},{"location":"internals/#Composition-1","page":"Internals","title":"Composition","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"h = f ∘ g","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"for some deterministic function g is the composition of f with g. i.e. h(x) = f(g(x)).","category":"page"},{"location":"internals/#conditioning-1","page":"Internals","title":"conditioning","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"h = g | (f(x) ← y)","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"should be read as h is the posterior process produced by conditioning the process g on having observed f at inputs x to take values y.","category":"page"},{"location":"internals/#approximate-conditioning-1","page":"Internals","title":"approximate conditioning","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"TODO (implemented, not documented)","category":"page"},{"location":"internals/#cross-1","page":"Internals","title":"cross","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"TODO (implemented, not documented)","category":"page"},{"location":"internals/#GPC-1","page":"Internals","title":"GPC","text":"","category":"section"},{"location":"internals/#","page":"Internals","title":"Internals","text":"This book-keeping object doesn't matter from a user's perspective but, unfortunately, we currently expose it to users. Fortunately, it's straightforward to work with. Say you wish to construct a collection of processes:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"# THIS WON'T WORK\nf = GP(mf, kf)\ng = GP(mg, kg)\nh = f + g\n# THIS WON'T WORK","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"You should write","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"# THIS IS GOOD. PLEASE DO THIS\ngpc = GPC()\nf = GP(mf, kf, gpc)\ng = GP(mg, kg, gpc)\nh = f + g\n# THIS IS GOOD. PLEASE DO THIS","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The rule is simple: when constructing GP objects that you plan to make interact later in your program, construct them using the same gpc object. For example, DON'T do the following:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"# THIS IS BAD. PLEASE DON'T DO THIS\nf = GP(mf, kf, GPC())\ng = GP(mg, kg, GPC())\nh = f + g\n# THIS IS BAD. PLEASE DON'T DO THIS","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The mistake here is to construct a separate GPC object for each GP. Hopefully, the code errors, but might yield incorrect results.","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"Alternatively, if you're willing to place your model in a function you can write something like:","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"@model function foo(some arguments)\n    f1 = GP(mean, kernel)\n    f2 = GP(some other mean, some other kernel)\n    return f1, f2\nend","category":"page"},{"location":"internals/#","page":"Internals","title":"Internals","text":"The @model macro places a GPC on the first line of the function and provides it as an argument to each GP constructed. Suggestions for ways to improve/extend this interface are greatly appreciated.","category":"page"}]
}
